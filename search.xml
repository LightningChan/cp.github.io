<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TextSnake]]></title>
    <url>%2F2018%2F12%2F22%2FTextSnake%2F</url>
    <content type="text"><![CDATA[TextSnake，是一个用于检测任意形状文本的灵活表征。 Motivation现有的方法中，有一个共同的假设，文本行实例的形状大体上是线性的，因此，可以采用相对简单的表征方式进行描述，如 Fig 1 中（a）轴对齐矩形,（b）旋转矩形，（c）四边形。然而，在面对带有透视形变（perspective distortion）的弧形文字 （curved text），这些表征方法在精确估计几何属性方面会有所欠缺，而且会引入更多的背景无关区域。 Figure 1. Comparison of different representations for text instances 因此，作者提出 TextSnake，可以适应各种形状的文本实例。本质思想使用凸 N 边形去包裹文本实例，论文结合极坐标很好的实现出来了。 MehthodRepresentationFigure 2. TextSnake representationTextSnake 将一个文本区域（黄色）表征为一系列有序而重叠的圆盘（蓝色），其中每个圆盘都由一条中心线（绿色，即对称轴或骨架）贯穿，并带有可变的半径 r 和方向 θ 。直观讲，TextSnake 能够改变其形状以适应不同的变化，比如旋转，缩放，弯曲。圆盘并非一一对应于文本实例的字符。但是圆盘序列的几何属性可以改正不规则形状的文本实例，并将其转化为对文本识别器更加友好的矩形等。#### PipelineFigure 3. Method framework: network output and post-processing 为检测任意形状的文本，本文借助 FCN 模型预测文本实例的几何属性。基于 FCN 的网络预测文本中心线（TCL），文本区域（TR）以及几何属性（包括 r，cosθ，sinθ）的分值图。由于 TCL 是 TR 的一部分，通过 TR 而得到 Masked TCL。假定 TCL 没有彼此重合，需要借助并查集（disjoint set）执行实例分割。Striding Algorithm 用于提取中心轴点，并最终重建文本实例。 ArchitectureFigure 4. Network Architecture. Blue blocks are convolution stages of VGG-16在 FPN 和 U-net 的启发下，本文提出一个方案，可逐渐融合来自主干网络不同层级的特征。主干网络可以是用于图像分类的卷积网络，比如 VGG-16/19 和 ResNet。这些网络可以被分成 5 个卷积阶段（stage）和若干个额外的全连接层。本文移除全连接层，并在每个阶段之后将特征图馈送至特征融合网络。#### Inference经过前向传播之后，网络输出 TCL，TR 以及几何图。对于 TCL 和 TR，阈值分别设为 T_tcl 和 T_tr；接着，TCL 和 TR 的交叉点给出 TCL 最后的预测。通过并查集，可以有效把 TCL 像素分割进不同的文本实例。最后，Striding Algorithm 被设计以提取用来表示文本实例形状和进程（course）的有序点，同时重建文本实例区域。Figure 5. Framework of Post-processing Algorithm Striding Algorithm 的流程如 Fig 5 所示。它主要包含 3 个部分：Act(a)Centralizing ，Act(b) Striding 和 Act(c)Sliding 。首先，本文随机选择一个像素作为起点，并将其中心化。接着，搜索过程分支为两个相反的方向——striding 和 centralizing 直到结束。这一过程将在两个相反方向上生成两个有序点，并可结合以生成最终的中心轴，它符合文本的进程，并精确描述形状。 ExperimentsTotal-Text &amp; CTW1500 数据集上展开的是有关曲形文本的实验，其优异表现证明了TextSnake 在处理曲形文本方面的有效性。 ICDAR 2015 上进行的是有关偶然场景文本的实验。在单一尺度测试中，TextSnake 超越了绝大多数现有方法（包括那些在多尺度中评估的方法），这证明了 TextSnake 的通用性以及已经可用于复杂场景的多方向文本。 本文在 MSRA-TD500 上进行有关长直文本线的实验。其中 TextSnake 的 F 值 78.3% 优于其他方法。 Discussion 想法十分新颖，TextSnake 对文本实例的精确描述具有强大的能力，因此能够带来很大的提升。 论文中提到，TextSnake 进行表征还有另外一个好处，能够很方便的转换成规范的形式，有利于后续的文本识别过程。 后处理操作过于繁琐，并且顶端的边界包裹不准确。 References [1] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2] blog: 旷视科技提出TextSnake：检测任意形状文本的灵活表征]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shape Robust Text Detection with Progressive Scale Expansion Network]]></title>
    <url>%2F2018%2F12%2F22%2FShape-Robust-Text-Detection-with-Progressive-Scale-Expansion-Network%2F</url>
    <content type="text"><![CDATA[针对任意形状文字检测，论文提出Progressive Scale Expansion Network (PSENet)，通过预测不同尺寸的文字区域的 kernels，然后采用 Breadth-First-Search 的方法从最小尺寸的 kernel 进行逐渐扩张到最大尺寸的 kernel。 ArchitecturePSENet 使用 Resnet 作为 backbone，结合 FPN，将不同层次的特征上采样到同样大小，然后拼接起来，预测不同尺寸的 kernels。最后通过 Progressive Scale Expansion 算法去不断地扩大 kernel，从而得到每个文字块。 PSENet 整体结构 Progressive Scale Expansion AlgorithmPSE 主要基于宽度优先搜索算法，先通过最小的 kernel 分割图，得到 connected components，此时，由于 kernel 较小，得到的是文本块的中心区域，能够将邻近的文本实例区分开，但是边界信息不准确。因此，将 CCs 中的元素压如队列，再依次和大尺寸的 kernel 分割结果合并。 My Thoughts 基于分割的方法，对于邻近的文本实例不容易分开，逐渐扩充的想法，简单而有效。 论文中生成 label 的方法可以参考借鉴。 References [1] [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PixelLink]]></title>
    <url>%2F2018%2F12%2F22%2FPixelLink%2F</url>
    <content type="text"><![CDATA[PixelLink 使用实例分割（Instance Segmentation）来实现文本检测，首先分割出文本区域，然后直接找出文本框，完全摒弃位置回归（Location Regression）的思想。 ArchitecturePixelLink 进行两种像素级别的预测：text/non-text 和 link 预测。标注时，在文本实例内部像素标为 positive，其余的标为 negative。Link 启发于 SegLink，但有所不同，每个像素点有 8 个邻居。给定一个像素点以及它的一个邻居点，如果它们同属于一个文本实例，则它们之间的 link 为 positive。 Figure 1. Structure of PixelLink+VGG16 2s PixelLink 将 VGG-16 结构作为基础网络，将后两层全连接转化为卷积，然后仿照类似 FPN 的结构，融合多层语义信息，最后得到原图二分之一大小的特征图。对于 link 的输出 feature map，其排列的顺序为：top-left，top，top-rignt，left，right，bottom-left，bottom，bottom-right。总的输出维度为 18 。 最后得到 pixels 和 links，然后根据link positive 将pixel positive 进行连接，得到 Cs(conected compoents)集合，集合中的每个元素代表的就是文本实例。两个 pixel 需要连接的前提条件：two link中至少有一个link positive．连接的规则采用的是 Disjoint set data structure (并查集)的方法。 基于上述 CCs 集合，直接掉调用 opencv 的 minAreaRect 提取带方向信息的矩形框。在此之后，还要根据在训练集上统计的信息，进行过滤，去掉噪声。 Loss由于文本行的长宽比变化范围广泛，若在计算loss的时候，对所有的 pixel positive 给予相同的权重，这对小面积的文本行是非常不公平的，针对上述问题，论文中提出了Instance-Balanced Cross-Entropy Loss。 My Thoughts 与CTPN，EAST，SegLink相比，PixelLink 对感受野的要求更少，因为每个神经元值只负责预测自己及其邻域内的状态。但是，会不会由于缺乏上下文信息，导致出现假阳性？ PixelLink 不需要用 imagenet 数据集训练的权重进行初始化。 与 SegLink 一样，不能检测很大的文本，这是因为link主要是用于连接相邻的segments，而不能用于检测相距较远的文本行。 后处理部分，严重依赖对数据集统计信息，对实验效果影响占比很大。 References [1] [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation paper code]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-organized Text Detection with Minimal Post-processing via Border Learning]]></title>
    <url>%2F2018%2F12%2F22%2FSelf-organized-Text-Detection-with-Minimal-Post-processing-via-Border-Learning%2F</url>
    <content type="text"><![CDATA[论文提出了基于分割的文本行级别的检测算法，通过引入 text border，再经过极少的后处理操作就能得到文本框。 Motivation根据检测单元的不同，相关算法可以分为： component detection：检测字符的一部分，再通过后处理连成检测框。 character detection：字符是单词的最小组成单元。 word detection：单词是常用的标准。 line detection：符合人类的阅读习惯。 region detection：能够排除很多 fasle positive。 其中，component/character 检测可划分为 bottom-up 的方法，而 region 检测为 up-down 的方法。通常，word/line 层次的检测框是想要的最终的结果，那么，直接把候选框就定为 word/line 级别则更方便。另外，现在许多基于分割的方法，后处理的操作过于复杂，且不是端到端的训练。因此，论文对于每个像素，进行 text，non-text 和 border 预测，最后通过简单的处理操作就能得到文本框。 MethodArchitecture算法采用自行设计的 FCN，最后输出 text map, non-text map 和 border map。 Figure 2. Fully convolutional networks used in the proposed method. Left: single resolution FCN. Right: multi-resolution FCN.#### Decoder得到最终的概率图后，先根据 text/non-text 得到 connected components，然后使用训练时 border c 的参数放大，得到检测框，具体步骤如下： My Thoughts 论文发现仅依靠 text region 是不足以区分邻近的实例。指出了两个重要问题：why are line-level annotations not sufficient？what can be done to resolve this problem？许多基于分割的方法，都在分析解决这两个问题。 border 确实是个好想法，能把帮助分开不同文本实例，而且后处理操作也简单，同样有篇利用 border 信息的论文[2]。 模型的训练数据是作者们提出的 PPT dataset，但是在 ICDAR 数据上进行训练时，没有讲清楚如何得到 border 标注信息。 另外，需要注意的是，论文里的实验都是基于文本行级别进行的。 References [1] [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning paper code [2] [2017-ICIP] WordFence: Text Detection in Natural Images with Border Awareness paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild]]></title>
    <url>%2F2018%2F12%2F22%2FMulti-scale-FCN-with-Cascaded-Instance-Aware-Segmentation-for-Arbitrary-Oriented-Word-Spotting-In-The-Wild%2F</url>
    <content type="text"><![CDATA[论文基于 FCN 分割的方法，结合 coarse-to-fine 的思想，先得到 text regions，再进行实例级分割得到文本框。 Pipeline 首先，使用 multi-scale FCN 提取图像中的 text regions，对于每个 text region 进行裁剪，依次送进 TL-CNN 中得到其所有的 text center lines，再依次将 text center line 和裁剪的原图合并送进 IA-CNN 进行分割，得到最终的文本框。 ArchitectureMutil-scale FCN 简单来说，就是送进网络训练的图像尺寸不一样，最后把所有的结果进行联合，每个尺寸的损失值也要进行计算，但是不同分支是共用卷积层的。 Multi-FCN architecture Instance segmentation 主要包括 TL-CNN 和 IA-CNN，其方法如下： Method of instance segmentation My Thoughts 想法有点类似于 Faster RCNN 的两阶段处理，只不过这里都是用 FCN 来分割，后来的许多方法是基于 Mask RCNN，其主要思路也是先粗后细。 Multi-scale 的做法，是不是可以在数据预处理进行 scale 的操作来替代，以减少网络训练的复杂度。 论文里提到几个有趣的观察，text block 召回的 false positive，text center line 可以过滤掉。还有就是有些字符的特征很难和背景噪声区分，如“I”，但并不意味字符特征就没用。 References [1] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EAST]]></title>
    <url>%2F2018%2F12%2F19%2FEAST%2F</url>
    <content type="text"><![CDATA[EAST 是一个简洁而高效的文字检测算法，避免了繁琐的后处理方法，直接用 FCN 进行像素级的预测和回归文字框。 ArictectureEAST 使用 PVANet，并借鉴 U-shape 的设计思想，融合高层和底层特征进行预测。Score map 的输出范围为[0,1]，并且代表着基于此像素点预测文本框的置信度。位置信息，可以选择 RBOX 和 QUAD 进行表示。后处理操作包括 Tresholding 和 NMS. Label Generation Score map label 生成如（a）所示，黄色虚线框是人工标注框，绿色框由黄色虚线框边长缩小0.3倍相邻最短边形成的，标注结果如（b）所示。 假设采用 RBOX 标注方式，先生成黄色虚线框的最小外接矩形框（红色），然后计算 score map 中的点分别到四条边的距离，以及红色矩形框的角度。 Limitations论文中，提到了 EAST 的两个缺点： 受限于感受野的大小，不能够很好检测 long text； 对于垂直文本的检测效果不好。 另外，EAST 对于边界的回归也不是很准确。 My Thoughts 针对于感受野小的问题，可以尝试改变卷积核尺寸，宽高比，以及使用 ASPP 等技巧； 对于边界回归不准，可以参考 AdvancedEAST[2]，也可以在训练 geometry 时直接计算点到边的相对距离，这样应该学到更准确的边界信息。 References [1] [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code [2] AdvancedEAST]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scene Text Detection via Holistic, Multi-Channel Prediction]]></title>
    <url>%2F2018%2F12%2F18%2FScene-Text-Detection-via-Holistic-Multi-Channel-Prediction%2F</url>
    <content type="text"><![CDATA[文字区域具有很强的边缘特征，因此论文修改 HED 对图像进行密集预测，输出 text region map, charcter map 以及 linking orientation map，最后融合三个 feature map 生成文本框。 Motivation场景文字检测中，简单进行 text/non-text 分割是不够充分的，有许多文本行距离很近，不能够完全的分开。因此，作者引入更多的监督信息，如字符的位置，大小以及连接方向，最后通过图分割（graph partition）得到文本线（text lines）。 MethodPipeline通过模型得到 text region map，character map，linking orientation map。 Figure 1. Pipeline of the proposed algorithm. (a) Original image. (b) Prediction maps. From left to right: text region map, character map and linking orientation map. (c) Detections. 每个 text region 上的 character 作为顶点，character 之间的相似性作为边，构建图模型，用最大生成求最小割，得到每个文本线。相似性包括空间相似性和方向相似性。空间相似性要求同一个文本实例间的 character 距离相近。方向相似性要求每两个 character 形成的直线方向与 linking orientation map 预测的方向接近。最后，将五个层的预测图经过 1*1 的卷积得到 3 个 maps，跟 ground Truth 计算损失值。 ArchitectureFigure 2. Network architecture of the proposed algorithm 网络的 Stage1~Stage 5 是 VGG16 的前五层，每个 stage 接一个 side-output 输出 3 个 response map，将 stage2~5 后面接 deconvolution 得到原图大小尺寸的预测图。 My Thoughts 论文提出的后处理的方法主要是为了得到文本线，文本线也是可以直接预测的，再结合一些方法，就不用如此复杂的后处理了。 论文中提到下一步可能会预测 character shapes 的二值掩码，能够有利于后续的识别步骤。 References [1] [2016] Scene Text Detection via Holistic, Multi-Channel Prediction paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Accurate text localization in natural image with cascaded convolutional text network]]></title>
    <url>%2F2018%2F12%2F18%2FAccurate-text-localization-in-natural-image-with-cascaded-convolutional-text-network%2F</url>
    <content type="text"><![CDATA[论文采用 coarse-to-fine 的思想，采用两个级联的网络，对于难以区分开不同文本实例的 text region，进行精细识别，得到更准确的结果。 Motivation基于 bottom up 思路的文字检测流程： 用滑动窗口或 MSER/SWT 等方法提取候选区域； 字符级分类器（SVM，CNN等）； 后处理，如文本线生成，字符聚类，字符分组，单词切割等。 然而这种方法存在一些缺点： 使用低级特征，对于光照不均匀，形变较大等目标无法有效提出候选区域； 候选区域很多，分类效率低； 后处理往往比较复杂，规则多，而且不通用； 多步流程容易造成误差累积，导致性能下降。 因此，作者采用 top-down 的思路，直接用 CNN 预测 text region 和 text center linev area。 MethodPipeline算法流程如下： ArchitectureCoarse text network 和 Fine text network 采用 VGG16 的网络结构，并进行了如下修改： 卷积核的尺寸变为多形状，并且是并行的。 去掉全连接层，加入两个卷积层。 多个层的特征进行融合。 Figure 2. Architecture 对于 coarse text network，只用了 text region 的监督信息。而 fine text network 用了 text line area（text region） 和 text center line area的监督信息，两者区别如 Fig3 所示。 Text center line area 的中心线处为1，逐渐向上下扩展，用高斯分布逐渐递减，半径为整个 bbox 高度的 1/4。因此，text center line area 实际上是包含了文本线的位置和文本块的高度信息。 Figure 3. Central line area (middle) and text line area (right) My Thoughts 采用 coarse-to-fine 思想，使用两个网络，参数变多，中间也要设定规则判断是否要输入下个网络，计算流程复杂。 提出的 text center line area 监督信息，使用高斯渐变标注高度信息，很有创新，能解决一些分割不开的情况。 References [1] [2016—CVPR] Accurate text localization in natural image with cascaded convolutional text network paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-Oriented Text Detection with Fully Convolutional Networks]]></title>
    <url>%2F2018%2F12%2F18%2FMulti-Oriented-Text-Detection-with-Fully-Convolutional-Networks%2F</url>
    <content type="text"><![CDATA[检测单个字符容易受背景干扰，造成漏检或误检的情况，而检测文本框相对于单个字符来说，和背景的区分行更强。因此，能不能结合局部信息（单个字符）和上下文信息（文本块）结合起来，使得检测更加鲁棒性。 FCN 在 2015 年被提出去进行像素分割，那么自然地想到能够标定每个像素属于文字的概率（salient map），也可以得到每个像素是字符中心的概率（centroid map）。 Methods算法流程如下图所示，主要分为：Text Block Detection, Text Line Generation 和 Text Line Candidates Classification。 Text Block Detection使用 Text-Block FCN 得到 salient map，然后对其进行过滤，聚类得到文本块。Text-Block FCN 的结构是在 VGG16 的基础上修改的，融合前几层的特征进行预测。 Text Line Generation步骤大体如下： Character Components Extraction：使用 MSER 方法提取字符候选区域，然后利用候选区域的面积和长宽比过滤噪声。 Orientation Estimation：这里假设同一个块中的所有文本线方向是一致的，文本线是近直线的。可以通过寻找最优的垂直坐标偏移 $h$ 和角度 $\theta$ 确定一条一线，使得该直线穿过字符数最多。公式如下，其中 $\Phi$ 是穿过字符的个数。$$\theta_{r}=\arg \max_{\theta} \max_{h} \Phi(\theta ,h)$$ Text Line Candidate Generation: 对每个块里的所有字符进行聚类，然后为每个类生成一个 bounding box。 Text Line Classification通过 Character-Centroid FCN 获得每条文本线中的所有可能存在的字符的中心。然后通过 Intensity criterion 和 Geometric criterion 过滤非文本线。 My Thoughts 这是较早将分割的思想引入到场景文字检测中，但还是用 MSER 方法提取字符，然后聚类，处理过程繁琐，text blocks 只是用来过滤 false positive 的情况。 References [1] [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scene Text Detection and Recognition Paper List]]></title>
    <url>%2F2018%2F12%2F18%2FScene-Text-Detection-and-Recognition-Paper-List%2F</url>
    <content type="text"><![CDATA[Collect and Record some excellent works on scene text detection and recognition. It will keep updating. If you have any suggestions about how to organize these papers, please contact me ! 1. Survey [2018] Scene Text Detection and Recognition: The Deep Learning Era paper [2016-TIP] Text detection, tracking and recognition in video: A comprehensive survey paper [2016-FCS] Scene text detection and recognition: Recent advances and future trends paper [2015-TPMAI] Text detection and recognition in imagery: A survey paper 2. Detection2.1 MethodsConventional [2010-CVPR] Detecting text in natural scenes with stroke width transform paper [2012-CVPR] Detecting Texts of Arbitrary Orientations in Natural Images paper Anchor Based [2018-CVPR] Geometry-Aware Scene Text Detection With Instance Transformation paper code [2018-AAAI] Feature Enhancement Network: A Refined Scene Text Detector paper [2017-AAAI] TextBoxes: A Fast Text Detector with a Single Deep Neural Network paper code [2017-CVPR] Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection paper [2017-ICCV] Single shot text detector with regional attention [2017-ICCV] Deep direct regression for multi-oriented scene text detection [2016-TIP] Text-Attentional Convolutional Neural Networks for Scene Text Detection [2015-CVPR] Symmetry-based text line detection in natural scenes paper Segmentation Based [2018-ECCV] Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping paper [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation papercode [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild paper [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning paper code [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments paper code [2017] Fused Text Segmentation Networks for Multi-oriented Scene Text Detection paper [2016] Scene Text Detection via Holistic, Multi-Channel Prediction paper [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks paper [2016-CVPR] Accurate text localization in natural image with cascaded convolutional text network paper Others [2019-AAAI] Scene Text Detection with Supervised Pyramid Context Network paper [2019-WACV] Mask R-CNN with Pyramid Attention Network for Scene Text Detection paper [2018-CVPR] Learning Markov Clustering Networks for Scene Text Detection paper [2016-ECCV] Detecting Text in Natural Image with Connectionist Text Proposal Network paper code 2.2 Specific TargetsMulti-Oriented Text [2018-IJCAI] IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection paper [2018-CVPR] Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation paper [2018-CVPR] Rotation-Sensitive Regression for Oriented Scene Text Detection paper [2017] Arbitrary-oriented scene text detection via rotation proposals paper [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code Irregular Text [2018-12] TextField: Learning A Deep Direction Field for Irregular Scene Text Detection paper [2018-11] TextMountain: Accurate Scene Text Detection via Instance Segmentation paper [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation papercode [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper [2017] Detecting Curve Text in the Wild: New Dataset and New Solution paper code [2017] Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition paper code Long Text [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments paper code 3. Recognition3.1 CTC based methods [2017-TPAMI] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition paper code 3.2 Attention based [2019-AAAI] Scene Text Recognition from Two-Dimensional Perspective paer [2019-11] Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition paper [2018-TPAMI] ASTER: An Attentional Scene Text Recognizer with Flexible Rectification paper code [2018-AAAI] Char-Net: A Character-Aware Neural Network for Distorted Scene Text paper [2018-AAAI] SqueezedText: A Real-time Scene Text Recognition by Binary Convolutional Encoder-decoder Network paper [CVPR-2016] Robust Scene Text Recognition with Automatic Rectification paper 3.3 Others [2019-AAAI] Scene Text Recognition from Two-Dimensional Perspective paper [2018-CVPR] Edit Probability for Scene Text Recognition [2018-CVPR] AON: Towards Arbitrarily-Oriented Text Recognition paper 4. End2End Recognition [2018-ECCV] Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes paper [2018-CVPR] An end-to-end TextSpotter with Explicit Alignment and Attention paper code [2018-CVPR] FOTS: Fast Oriented Text Spotting with a Unified Network paper [2018-AAAI] SEE: Towards Semi-Supervised End-to-End Scene Text Recognition paper [2017-ICCV] Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework paper code [2017-ICCV] Towards end-to-end text spotting with convolutional recurrent neural networks. paper [2018-TIP] TextBoxes++: A Single-Shot Oriented Scene Text Detector. paper code 5. Auxilliary Techs5.1 Synthetic Data [2018-ECCV] Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes paper [2016-CVPR] Synthetic Data for Text Localisation in Natural Images paper 5.2 Bootstrapping [2018-ECCV] Wordsup: Exploiting word annotations for character based text detection [2017-ICCV] Wetext: Scene text detection under weak supervision 5.3 Context Information [2018-ECCV] Using Object Information for Spotting Text paper 5.4 GAN [2018-ECCV] Synthetically Supervised Feature Learning for Scene Text Recognition paper 6. Unsorted [2018-09] TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade paper [2018-10] Correlation Propagation Networks for Scene Text Detection paper There are also other helpful resources: Awesome-Scene-Text-Recognition OCR Resuourses image-text-localization-recognition SceneTextPapers]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text</tag>
      </tags>
  </entry>
</search>
