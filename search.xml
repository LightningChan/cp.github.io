<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Deeptracker 第二期]]></title>
    <url>%2F2019%2F08%2F11%2FDeeptracker-20190811%2F</url>
    <content type="text"><![CDATA[Deeptracker 旨在于搜集关于 Deepfakes 的技术动态、新闻报道等信息，让研究者们更多地关注 AI 技术的滥用。本人也正在研究如何利用深度学习技术帮助鉴别音视频内容是否被篡改，欢迎感兴趣的童鞋一些交流讨论！ 技术动态【1】一项调查发现，在网络市场上，中国deepfake盗版色情作品的价格仅为2美元 《京华时报》和《环球时报》的一项调查发现，网络市场上存在大量非法的deepfake色情作品，其中不乏中国知名女性明星的视频。 此次调查由《新京报》的记者牵头，他们在二手交易市场“闲鱼”和百度论坛上发现了deepfake色情作品的销售。在这两个网站上，中国女明星的deepfake色情视频被发现捆绑销售，价格从2美元几十个视频到25美元数百个视频不等。记者们还发现deepfake网站上有卖家提供定制视频服务的广告，包括可以让买家选择主题和提供自学教程。 调查显示，与在美国和欧洲看到的情况一样，deepfake在中国也有类似的发展和商业化。色情作品目前在中国是非法的，中国立法者已经采取行动，禁止所有侵犯个人肖像权的视频。在上述调查之后，两家网站都声称已经删除了所有涉及deepfake的非法列表，并实施了对deepfake和faceswap等关键词的搜索禁令。然而，与Pornhub等色情网站一样，这些禁令的尝试并没有取得成功，类似的搜索结果在仍然会出现。 【2】Faceapp的照片老化功能标志着人工智能合成媒体的病毒式传播 智能手机照片编辑应用Faceapp发布了一项老化功能，用户可以看到30年后自己的样子，该应用使用人工智能技术，合成的图像在全球掀起了一股热潮。 2017年，在机器学习的reddit分论坛上，一场讨论表明，这款应用结合了GANs和更经典的电脑视觉技术，来生成逼真的照片修改。开源软件可以创建类似于Faceapp的输出，它使用英伟达的StyleGAN作为骨干，可以在人脸编码中找到向量，这些向量对应于抽象特征，比如年龄、性别、头发长度。虽然很难确定FaceApp实现的详细技术，但很可能他们正在使用与StyleGAN类似的GANs来计算合成衰老和其他面部变化所需的编码。 尽管这款应用在2017年发布时添加了许多不同的编辑功能，但这款应用最近的病毒式传播要归功于合成老化人脸的功能，该功能因其真实性和准确性而受到好评。尽管该应用的使用条款仍存在争议，但上周该应用在美国的下载量增长了560%，数据库中有超过1.5亿张脸。Faceapp也从侧面表明了用于生成合成媒体的人工智能工具正在逐渐商品化。 【3】麻省理工学院和IBM允许用户将一张照片转换成15世纪的肖像图 麻省理工学院和IBM沃森实验室的研究人员创建了一个人工智能肖像网站，该网站能够依据一张生活照片创建出风格类似于15世纪的肖像画。 该项目基于GANs实现，GANs识别给定照片的面部线条和面部特征，生成一个15世纪风格的全新面孔。基于GAN的算法使用4.5万幅经典肖像画的数据集进行训练，并且着重1400幅画作(由于该时期在西方被誉为写实肖像画的起点)，还包括一些文艺复兴早期和当代的画作。输出的是一幅4k合成图像，逼真地“绘制”了一幅古代大师风格的肖像。从一些例子中可以看出，作品在捕捉个人形象的同时，也将某些特征转化为与肖像画风格相一致的内容。 该项目在Faceapp的老龄化功能病毒式传播后不久就开始流行起来，该功能也可能使用GANs生成最终输出，提高合成结果的准确性。然而，创建者表示，该项目旨在让用户考虑算法偏见如何影响决策和输出。这是因为人工智能人像只针对特定的西方绘画进行训练，这意味着生成的图像符合非常特定的西方风格，不考虑来自不同文化的其他不同风格。设计者还鼓励用户上传他们微笑或露出牙齿的照片进行实验，因为15世纪的肖像画中很少包含微笑主题，这意味着生成的图像可能会以有趣的方式扭曲或不准确。 【4】印度政客在成为deepfake色情的目标后“崩溃” 据英国《每日邮报》报道，印度政界人士林巴伐利在新德里立法议会进行了含泪抗议。此前，有人利用 deepfake 伪造他进行同性恋性行为的视频。 林巴伐利要求对一周前发布的所谓的“deepfake”视频进行官方调查，该视频描述了他与同性恋者发生性关系。林巴伐利声称“很难想象你的家人受到了如此严重的精神创伤”。一位前政界人士也提到了虚假视频对印度政界人士的影响，其中许多视频在社交媒体和Whatsapp 群组上流传。演讲结束后，警方报告称已提起诉讼，并已开始努力查明在社交媒体上创建或分享这些视频的人。 这起案件发生在马来西亚和巴基斯坦的几起案件之后，在这两国，政客们受到诽谤，或声称自己被描绘成同性恋或不当性关系的deepfake视频所玷污。尽管deepfake色情作品的绝大多数针对的是女性，但这些案件突显出deepfake色情作品可能造成的损害。色情作品描绘的是男政客在同性恋性场景中的行为，在这些国家，此类行为是非法的，或者仍然受到严重歧视。由于印度已经饱受虚假信息泛滥之苦，针对政客和边缘化群体的deepfake，可能会显著加剧现有形式的虚假信息引发的频繁暴力反应。 【5】美国众议院情报委员会主席敦促大型科技公司在2020年美国大选之前出台打击deepfake政策 美国众议院情报委员会主席亚当·希夫致信Twitter、Facebook和谷歌的首席执行官，敦促他们澄清打击deepfake的政策和战略。 这些信件都是直接写给每家公司的首席执行官的，并就他们“如何在以病毒营销为核心特征的平台上，对deepfake的潜在危害做出有效回应”寻求答案。这些问题包括: 查看和分享南希•佩洛西的shallowfake的指标，该平台在打击deepfake方面有哪些具体政策，该平台是否已经禁止假冒图片或视频，以及该平台是否正在开发检测deepfake和其他虚假内容的技术。 此前，美国众议院情报委员会于今年6月举行了一场关于deepfake的公开听证会，希夫对deepfake所构成的危险表达了越来越强烈的担忧，这种担忧将一直持续到2020年美国总统大选。尽管国会目前正在审议几项deepfake法案，包括《恶意deepfake禁令法案》和《deepfake问责法案》，但这些信件表明，人们越来越关注社交媒体平台和关键的网络服务的作用，认为它们在实际中对打击deepfake至关重要。在南希·佩洛西的shallowfake病毒式传播之后，这些平台因处理视频的方式受到了密切关注。希夫的信件表明，他们相信，政治压力是迫使形成强大的“deepfake”战略所必需的。 新闻速览 Poynter 的研究人员编制了一份清单，列出了7款最受欢迎的教育类游戏，旨在教给孩子们核查事实的基本要素，以及如何在网上发现错误信息。(Poynter) 一项被怀疑是俄罗斯所为的行动被确认为首次利用政治紧张局势，散布有关北爱尔兰的虚假信息和假新闻。(爱尔兰时报) 成人平台YouPorn开发了一个CGI色情演员Jedy Vales，该公司承诺将为该角色和其他合成模特制作完整的色情视频。 (Dazed Digital) 澳大利亚公司Skylight Cyber针对Cylance公司基于人工智能的杀毒软件，通过设计对抗性攻击，使其误以为WannaCry、SamSam和其他已知恶意软件是无害的。(Skylight Cyber) 网络安全专家的一份报告得出结论，美国2020年总统大选很可能成为包括伊朗在内的其他几个国家的虚假信息攻击目标。 (The Hill) Facebook前安全主管亚历克斯·斯塔莫斯收到了克雷格的“名单”创始人克雷格·纽马克捐赠的500万美元，用于成立斯坦福互联网观察中心，以研究互联网技术的滥用。(WIRED) 加州大学伯克利分校的研究人员建立了一个包含7500张“自然对抗图片”的数据库，这些“自然对抗图片”是真实生活中物体的普通未经编辑的照片，会在计算机视觉系统中造成非强迫错误。(The Verge) 《纽约时报》推出了一个新闻来源项目，帮助出版商探索打击虚假信息的技术手段，包括基于区块链的解决方案。(纽约时报) 观点讨论 维克多•利帕贝利概述了合成媒体公司Synthesia对合成媒体技术未来的愿景，重点是最大限度地发挥人类的创造潜力，将有害用途降到最低。page Denise Melchin探讨了禁止deepfake色情的法律对云计算服务的影响，以及这些服务如何阻止在其平台上创建deepfake色情。page Craig Silverman根据媒体专家的工作，提出了一个媒体框架用于教育老年人如何识别和检查网上虚假或极端内容的事实。page Nina Iacono Brown认为，美国国会试图在2020年大选前“解决”deepfake问题，从根本上低估了问题的复杂性和附带损害的风险。page]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Deepfakes</category>
        <category>Deeptracker</category>
      </categories>
      <tags>
        <tag>Deeptracker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeptracker 第一期]]></title>
    <url>%2F2019%2F07%2F12%2FDeeptracker-20190712%2F</url>
    <content type="text"><![CDATA[Deeptracker 旨在于搜集关于 Deepfakes 的技术动态、新闻报道等信息，让研究者们更多地关注 AI 技术的滥用。本人也正在研究如何利用深度学习技术帮助鉴别音视频内容是否被篡改，欢迎感兴趣的童鞋一些交流讨论！ 技术动态【1】饱含争议的DeepNude应用程序发布一天即关闭，用于合成裸体女性照片 这是一个富有争议的应用程序，从女性照片中“删除”衣服。该app已被其创建者在发布后不到一天删除，理由是服务器过载和道德问题。 这是一款windows和linux的应用程序，使用开源的GAN 模型 pix2pix。该app大约30秒内从目标照片中合成裸体照片。虽然该应用程序称无法从穿着上生成逼真的照片，但当主体在高质量的图片上显示出一些皮肤时，很容易产生逼真的“裸体”。该app可在6月23日免费下载，但生成照片覆盖水印。要删除大号水印，用户必须支付50美元。 由于其明显的恶意应用和缺乏道德监督，对该应用的媒体报道表达了强烈的抗议和厌恶。尤其是妇女权利活动家谴责该应用程序有可能大大加剧由深度学习造假色情制品的危害，并且这种武器化的技术易于获取。虽然该应用程序的创建者最初减持认为该应用不是用于色情用途，而是技术热情的产物，但他仍然删除了应用程序，理由是下载需求导致服务器过载与担心其被不合理应用。然而，他的结论性评论“世界还没有为DeepNude做好准备”表明删除应用程序可能不是永久性的。 【2】马克·扎克伯格承认Facebook需要重新考虑deepfake政策，以将其从“虚假新闻”中区别出来 马克·扎克伯格承认，Facebook需要重新考虑他们在阿斯彭创意节上处理deepfake的政策了，但他辩护说，在社交网络上留下了南希·佩洛西的shallowfake内容。 扎克伯格承认，将deepfakes与书面虚假新闻混为一谈是错误的，并指出有一个很好地案例可以证明“deepfakes实际上是完全不同于虚假新闻的事物”。然而，他强调需要仔细确定deepfakes的定义，以确保Facebook不会为人们提供对包含他们根本不喜欢的内容或仅仅略微修改过的内容的审查方法。他自己将deepfakes定义为“人工智能操作过的媒体”，尽管这个定义含糊不清，但会成为一个谨慎的占位符，因为Facebook将继续探讨这个问题。 政策的重新考虑并不意味着Facebook授权删除。尽管扎克伯格承认Facebook对南希·佩洛西的shallowfake反应缓慢（因为该视频需要花费一天以上的时间来标记），但他为这一视频的优先级和标签做出了辩护，而不是将其删除。他呼吁对Facebook现有的方法进行“改进执行”，但最终还是声称标准是适当移除监管的唯一方法。他的立场使得Facebook不是真理的仲裁者，他说：“私人公司阻止你说出一些他认为不正确的事情”。 【3】 华盛顿邮报发布识别不同类型伪造视频新指南 华盛顿邮报的一个团队制作了一份全面指南，用于识别不同类型的伪造视频，旨在教育读者、检查员和其他记者。 该团队首先分析了被伪造的数个小时的镜头，将其分为了20个不同的类别。他们又确定了进一步细化为6个关键术语和类别的共同元素及特征。包括三个一般类别（缺少上下文、欺骗性编辑和恶意转换），这些类别进一步细分为两个特定的子类别。每个类别和子类别都附有一个例子及相关文章，提供更加详细的信息。 该指南是一个强大的资源，为不同类型的视频操作提供了清晰而全面的分类。包含概述指南主要发现的视频，以及读者可以提交怀疑被修改的视频以供审查的表格，对于确保指南易读和参与度也很重要。展望未来，作者表示将更新指南以反映视频伪造的变化情况，这意味着这份指南有望不会过时。 【4】 一种新的生成技术：从单张图片和音频生成动态说话视频 来自三星AI中心和伦敦帝国理工学院的研究人员创造了一种技术，用于制作动态脸部视频，其唇部动作和面部表情与语音音频样本相匹配。 这项技术被称为”语音驱动的面部动画”，通过从目标人物的单张图片和语音音频样本自动合成动态面部视频。这项工作由cycle GAN来执行，该cycle GAN将来自音频的语音信号转换成与样本照片匹配的相应嘴部动作。此外，还从样本照片中生成眉毛移动和眨眼，使脸部动画更加自然生动。最终输出几个超现实的视频，视频中唱或说的语音与照片或历史人物高度匹配，例如拉斯普京”唱”光环，金正日”唱”江南style等。 这个链接页面中包含了图灵测试，供用户测试他们能否从生成的视频中辨别出真实的视频。尽管面部动画不算很逼真，但该技术使用单张照片和音频文件来生成人脸确实令人印象深刻。尤其是能够重现伴随语音音频的准确面部动作以及前面提到的眨眼/眉毛运动。 新闻速览 一个现实的合成“虚拟影响者”在Instagram上积累了超过160万粉丝，并被用于宣传几个着名的时尚品牌，包括普拉达和凯文克莱因。(NYT) Deepfake的创始人Villainguy发布了一段演员罗斯·马昆德(Ross Marquand)“变脸”的Deepfake视频，以匹配他在吉米·坎摩尔秀(Jimmy Kimmel show)上模仿的11位名人。(The Daily Dot) 谷歌首席执行官承认，Youtube一直在努力对抗平台上仇恨言论和虚假信息的突出，并承认他们永远无法过滤100%的有害内容。(CNN) 香港的抗议活动导致网上虚假信息激增，中国官方新闻发布了误导性文章，一些肤浅的虚假图片在社交媒体上疯传。(Buzzfeed News) 人工智能合作组织发表了一篇博客文章，总结了他们最近在BBC举办的研讨会，主题是如何保护公共话语免受人工智能产生的错误/虚假信息的影响。(Partnership on AI) 美国参议员和众议院代表提出了两党合作的《深度造假报告法案》(Deepfake Report Act)，该法案要求国土安全部(Department of Homeland Security)每年对深度造假进行研究。 THE HILL 加州立法者还提出了一项法案，该法案规定，除非媒体被明确贴上虚假或被操纵的标签，否则在选举前60天故意传播深度造假将被视为非法。Techwire Instagram负责人亚当·莫塞里(Adam Mosseri)表示，该平台正在评估其处理深度假货的方法，强调“安全与言论之间的平衡”以及“更快获取内容”方面的困难。CBS 在哥本哈根民主峰会上，一段关于唐纳德·特朗普声称“上帝选我当总统”的deepfake“Skype电话”被播出，以警告这项技术带来的危险。The National 观点讨论 国会不应急于监管deepfakes？电子前沿基金会(EFF)的研究人员认为，美国目前提出的打击deepfakes的立法缺乏确保言论自由保护所必需的细微差别。page 当涉及到识别假照片时，经验比上下文线索更重要：蒙娜·卡斯拉(Mona Kasra)概述了她对影响识别假照片成功的因素的研究，包括发现在线经验和图像编辑知识对正确决策最重要。page 打击deepfake视频是每个人的责任：胡珍(Jane C Hu)认为，社交媒体平台在打击深度假货方面进展缓慢，这意味着个人必须承担起培养发现假货所需的媒体素养技能的责任。page 二次侵扰：一个可疑的俄罗斯情报行动，DFR实验室展示了他们对一场复杂的反西方虚假信息运动的最新研究。这场运动利用虚假账户和伪造文件，在数十个在线平台上瞄准用户。page 蒂芙尼·C·李(Tiffany C Li)分析了金·卡戴珊(Kim Kardashian)旗下deepfake最近因侵犯版权而被撤下的消息，并认为合理使用的例外情况使得版权法不适合作为解决deepfake问题的法律解决方案。page 编辑：邹书桥、陈鹏]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Deepfakes</category>
        <category>Deeptracker</category>
      </categories>
      <tags>
        <tag>Deeptracker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019 读书计划]]></title>
    <url>%2F2019%2F01%2F01%2F2019-%E8%AF%BB%E4%B9%A6%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[You can either travel or read, but either your body or soul must be on the way! Roman Holiday 社科类 菲利普·津巴多：《态度改变与社会影响》 哲学类 罗伯特·罗素：《西方哲学史》 王阳明：《传习录》 洪应明：《菜根谭》 胡适：《中国哲学史大纲》 历史类 黄仁宇：《中国大历史》 [Reading] 郭建龙：《穿越百年中东》 雅克·阿塔利：《未来简史》 林汉达：《中国历史故事集》 思维类 查理·芒格：《穷查理宝典》 沟通类 马歇尔·卢森堡：《非暴力沟通》 科里帕特森：《关键对话》 心理学类 凯利·麦格尼格尔：《自控力》 阿尔弗雷德·阿德勒：《理解人性》 戴维·迈尔斯：《社会心理学》 小说类 阿尔贝.加缪：《局外人》 乔治·奥威尔：《1984》 其他 华杉：《华杉讲透孙子兵法》]]></content>
      <categories>
        <category>读万卷书</category>
      </categories>
      <tags>
        <tag>阅读计划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask TextSpotter]]></title>
    <url>%2F2018%2F12%2F25%2FMask-TextSpotter%2F</url>
    <content type="text"><![CDATA[论文启发于 Mask RCNN，提出一种 end2end 的场景文字检测和识别模型，能够检测和识别任意形状的文本。 论文认为文献[2]和[3]虽然提出了 end2end 模型，但是训练的时候还是分步骤训练的，不能完全达到一个 end2end的行为。另外，它们只聚焦于水平或旋转的文本，不能解决弯曲的文本。 ArchitectureFigure 1. Model architecture 如 figure 1 所示，mask textspotter 由四部分组成： Backbone：FPN + ResNet50，用来提取特征图。 RPN：生成 text proposals，然后使用 RoI Align 提取 RoI 特征。 Fast RCNN：对 text proposals 进行更精确地分类和回归。 Mask Branch：包含 text instance 分割和 character 分割任务。如 figure 2 所示，会输出一个 global word map，36 个 character maps 和一个 background map。 Figure 2. Mask branch character 的 Bbox 被缩小成原来的四分之一。 Loss Function损失函数可以定义为：$$L = L_{rpn} + \alpha_1 L_{rcnn} + \alpha_2 L_{mask} \\\ L_{mask}=L_{global}+\beta L_{char}$$这里的 $\alpha_1,\alpha_2,\beta$ 都设为 1，其中 $L_{global}$ 是一个平均 binary cross-entropy 损失：$$L_{global}= - \frac{1}{N} \sum_{n=1}^{N} [y_n \times \log(S(x_n))+(1-y_n) \times \log(1-S(x_n))]$$这里的 $S(x)$ 是 sigmoid 函数。 $L_{char}$ 是一个加权的 soft-max 损失， $Y$ 是 $X$ 对应的 ground truth：$$L_{char}= - \frac{1}{N} \sum_{n=1}^{N} W_n \sum_{t=0}^{T-1} Y_{n,t} \log(\frac{e^{x_{n,t}}}{\sum_{k=0}^{T-1}e^{x_{n,k}}})$$ Inference测试将 Fast RCNN 产生的 Bbox 经过 NMS 后作为候选框输入 mask branch 生成 global maps 和 character maps。最后产生的 polygons 直接通过计算 global maps 得到。字符序列则通过 pixel voting 算法。 Pixel Voting 流程： 将 background map 进行二值化得到所有的字符区域； 对于所有的 character maps 求每个字符区域的像素得分均值，把得分最高的字符类别得分设为该区域输出的字符； 把所有的字符从左到右连接得到字符序列。 论文发现如果使用普通的编辑距离去匹配单词，会有多个的编辑距离一样，因此提出 weighted edit distance。 My Thoughts Mask TextSpotter 本质是用 Faster RCNN 进行目标定位，然后使用目标分割来识别字符，那么可以替换分割算法提升效果。 之前的 text spotter 模型都是先分开训练，再联合训练，这个能直接联合训练。 英文的字母较少，在中文上可能不行。 References [1] [2018-ECCV] Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes paper [2] [2017-ICCV] Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework papercode [3] [2017-ICCV] Towards end-to-end text spotting with convolutional recurrent neural networks. paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>End2end</category>
      </categories>
      <tags>
        <tag>Scene Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TextContourNet]]></title>
    <url>%2F2018%2F12%2F25%2FTextContourNet%2F</url>
    <content type="text"><![CDATA[TextContourNet 利用实例级别的场景文字的边界轮廓信息，帮助提高文字检测器的能力。 Text Contour这里的文本轮廓不是语义级别的，是针对每个文本实例。轮廓可以由边界框来生成，论文中使用一种平滑的轮廓信息来进行训练，其生成公式如下所示，其中 $S_{contour}$ 表示边界框上的像素点集合。 Architecture论文主要想论证，引入轮廓信息能够帮助常规的文本检测器提高检测效果。选用 EAST 模型作为基础文本检测器。并设计了几种不同的引入方法。 Auxiliary TextContourNet，将 contour task 作为 auxiliary loss。 Cascade TextContourNet 设计两种方案。第一种，两个分支不共享 encoder 模块，将得到的轮廓结果和原图拼接在一起，再进行检测。 第二种就是共享 encoder 模块，将得到的轮廓结果和检测分支的特征拼接在一起，再进行检测。 最后的实验结果表明，Cascade 模式下共享 encoder 的效果是最好的，可能的原因是在两种损失的联合下，encoder 部分训练得更好，可以再补充一个实验证明一下，即共享 encoder 模块，将得到的轮廓结果和原图拼接，进行检测。 My Thoughts 论文中提到的 Text Contour 其实和 Text Border 的概念是类似，早有工作将 border 信息引入场景文字检测中，确实发现有效果。 contour ground truth 采用平滑的方式，想法不错。 利用分割得到一些辅助信息，然后和检测分支拼接，帮助提高检测效果。有许多工作进行尝试，如文献[3]中，通过分割得到 text/non-text map，来消除背景噪声。文献[2]提取 text center line 再和原图拼接，进行检测，不过它是 crop 每个 text regions 进行的，比较繁琐。 References [1] [2018]TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade paper [2] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild paper [3] [2017-ICCV] Single shot text detector with regional attention paper code]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TextSnake]]></title>
    <url>%2F2018%2F12%2F22%2FTextSnake%2F</url>
    <content type="text"><![CDATA[TextSnake，是一个用于检测任意形状文本的灵活表征。 Motivation现有的方法中，有一个共同的假设，文本行实例的形状大体上是线性的，因此，可以采用相对简单的表征方式进行描述，如 Fig 1 中（a）轴对齐矩形,（b）旋转矩形，（c）四边形。然而，在面对带有透视形变（perspective distortion）的弧形文字 （curved text），这些表征方法在精确估计几何属性方面会有所欠缺，而且会引入更多的背景无关区域。 Figure 1. Comparison of different representations for text instances 因此，作者提出 TextSnake，可以适应各种形状的文本实例。本质思想使用凸 N 边形去包裹文本实例，论文结合极坐标很好的实现出来了。 MehthodRepresentationFigure 2. TextSnake representationTextSnake 将一个文本区域（黄色）表征为一系列有序而重叠的圆盘（蓝色），其中每个圆盘都由一条中心线（绿色，即对称轴或骨架）贯穿，并带有可变的半径 r 和方向 θ 。直观讲，TextSnake 能够改变其形状以适应不同的变化，比如旋转，缩放，弯曲。圆盘并非一一对应于文本实例的字符。但是圆盘序列的几何属性可以改正不规则形状的文本实例，并将其转化为对文本识别器更加友好的矩形等。#### PipelineFigure 3. Method framework: network output and post-processing 为检测任意形状的文本，本文借助 FCN 模型预测文本实例的几何属性。基于 FCN 的网络预测文本中心线（TCL），文本区域（TR）以及几何属性（包括 r，cosθ，sinθ）的分值图。由于 TCL 是 TR 的一部分，通过 TR 而得到 Masked TCL。假定 TCL 没有彼此重合，需要借助并查集（disjoint set）执行实例分割。Striding Algorithm 用于提取中心轴点，并最终重建文本实例。 ArchitectureFigure 4. Network Architecture. Blue blocks are convolution stages of VGG-16在 FPN 和 U-net 的启发下，本文提出一个方案，可逐渐融合来自主干网络不同层级的特征。主干网络可以是用于图像分类的卷积网络，比如 VGG-16/19 和 ResNet。这些网络可以被分成 5 个卷积阶段（stage）和若干个额外的全连接层。本文移除全连接层，并在每个阶段之后将特征图馈送至特征融合网络。#### Inference经过前向传播之后，网络输出 TCL，TR 以及几何图。对于 TCL 和 TR，阈值分别设为 T_tcl 和 T_tr；接着，TCL 和 TR 的交叉点给出 TCL 最后的预测。通过并查集，可以有效把 TCL 像素分割进不同的文本实例。最后，Striding Algorithm 被设计以提取用来表示文本实例形状和进程（course）的有序点，同时重建文本实例区域。Figure 5. Framework of Post-processing Algorithm Striding Algorithm 的流程如 Fig 5 所示。它主要包含 3 个部分：Act(a)Centralizing ，Act(b) Striding 和 Act(c)Sliding 。首先，本文随机选择一个像素作为起点，并将其中心化。接着，搜索过程分支为两个相反的方向——striding 和 centralizing 直到结束。这一过程将在两个相反方向上生成两个有序点，并可结合以生成最终的中心轴，它符合文本的进程，并精确描述形状。 ExperimentsTotal-Text &amp; CTW1500 数据集上展开的是有关曲形文本的实验，其优异表现证明了TextSnake 在处理曲形文本方面的有效性。 ICDAR 2015 上进行的是有关偶然场景文本的实验。在单一尺度测试中，TextSnake 超越了绝大多数现有方法（包括那些在多尺度中评估的方法），这证明了 TextSnake 的通用性以及已经可用于复杂场景的多方向文本。 本文在 MSRA-TD500 上进行有关长直文本线的实验。其中 TextSnake 的 F 值 78.3% 优于其他方法。 Discussion 想法十分新颖，TextSnake 对文本实例的精确描述具有强大的能力，因此能够带来很大的提升。 论文中提到，TextSnake 进行表征还有另外一个好处，能够很方便的转换成规范的形式，有利于后续的文本识别过程。 后处理操作过于繁琐，并且顶端的边界包裹不准确。 References [1] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2] blog: 旷视科技提出TextSnake：检测任意形状文本的灵活表征]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shape Robust Text Detection with Progressive Scale Expansion Network]]></title>
    <url>%2F2018%2F12%2F22%2FShape-Robust-Text-Detection-with-Progressive-Scale-Expansion-Network%2F</url>
    <content type="text"><![CDATA[针对任意形状文字检测，论文提出Progressive Scale Expansion Network (PSENet)，通过预测不同尺寸的文字区域的 kernels，然后采用 Breadth-First-Search 的方法从最小尺寸的 kernel 进行逐渐扩张到最大尺寸的 kernel。 ArchitecturePSENet 使用 Resnet 作为 backbone，结合 FPN，将不同层次的特征上采样到同样大小，然后拼接起来，预测不同尺寸的 kernels。最后通过 Progressive Scale Expansion 算法去不断地扩大 kernel，从而得到每个文字块。 PSENet 整体结构 Progressive Scale Expansion AlgorithmPSE 主要基于宽度优先搜索算法，先通过最小的 kernel 分割图，得到 connected components，此时，由于 kernel 较小，得到的是文本块的中心区域，能够将邻近的文本实例区分开，但是边界信息不准确。因此，将 CCs 中的元素压如队列，再依次和大尺寸的 kernel 分割结果合并。 My Thoughts 基于分割的方法，对于邻近的文本实例不容易分开，逐渐扩充的想法，简单而有效。 论文中生成 label 的方法可以参考借鉴。 References [1] [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PixelLink]]></title>
    <url>%2F2018%2F12%2F22%2FPixelLink%2F</url>
    <content type="text"><![CDATA[PixelLink 使用实例分割（Instance Segmentation）来实现文本检测，首先分割出文本区域，然后直接找出文本框，完全摒弃位置回归（Location Regression）的思想。 ArchitecturePixelLink 进行两种像素级别的预测：text/non-text 和 link 预测。标注时，在文本实例内部像素标为 positive，其余的标为 negative。Link 启发于 SegLink，但有所不同，每个像素点有 8 个邻居。给定一个像素点以及它的一个邻居点，如果它们同属于一个文本实例，则它们之间的 link 为 positive。 Figure 1. Structure of PixelLink+VGG16 2s PixelLink 将 VGG-16 结构作为基础网络，将后两层全连接转化为卷积，然后仿照类似 FPN 的结构，融合多层语义信息，最后得到原图二分之一大小的特征图。对于 link 的输出 feature map，其排列的顺序为：top-left，top，top-rignt，left，right，bottom-left，bottom，bottom-right。总的输出维度为 18 。 最后得到 pixels 和 links，然后根据link positive 将pixel positive 进行连接，得到 Cs(conected compoents)集合，集合中的每个元素代表的就是文本实例。两个 pixel 需要连接的前提条件：two link中至少有一个link positive．连接的规则采用的是 Disjoint set data structure (并查集)的方法。 基于上述 CCs 集合，直接掉调用 opencv 的 minAreaRect 提取带方向信息的矩形框。在此之后，还要根据在训练集上统计的信息，进行过滤，去掉噪声。 Loss由于文本行的长宽比变化范围广泛，若在计算loss的时候，对所有的 pixel positive 给予相同的权重，这对小面积的文本行是非常不公平的，针对上述问题，论文中提出了Instance-Balanced Cross-Entropy Loss。 My Thoughts 与CTPN，EAST，SegLink相比，PixelLink 对感受野的要求更少，因为每个神经元值只负责预测自己及其邻域内的状态。但是，会不会由于缺乏上下文信息，导致出现假阳性？ PixelLink 不需要用 imagenet 数据集训练的权重进行初始化。 与 SegLink 一样，不能检测很大的文本，这是因为link主要是用于连接相邻的segments，而不能用于检测相距较远的文本行。 后处理部分，严重依赖对数据集统计信息，对实验效果影响占比很大。 References [1] [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation paper code]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-organized Text Detection with Minimal Post-processing via Border Learning]]></title>
    <url>%2F2018%2F12%2F22%2FSelf-organized-Text-Detection-with-Minimal-Post-processing-via-Border-Learning%2F</url>
    <content type="text"><![CDATA[论文提出了基于分割的文本行级别的检测算法，通过引入 text border，再经过极少的后处理操作就能得到文本框。 Motivation根据检测单元的不同，相关算法可以分为： component detection：检测字符的一部分，再通过后处理连成检测框。 character detection：字符是单词的最小组成单元。 word detection：单词是常用的标准。 line detection：符合人类的阅读习惯。 region detection：能够排除很多 fasle positive。 其中，component/character 检测可划分为 bottom-up 的方法，而 region 检测为 up-down 的方法。通常，word/line 层次的检测框是想要的最终的结果，那么，直接把候选框就定为 word/line 级别则更方便。另外，现在许多基于分割的方法，后处理的操作过于复杂，且不是端到端的训练。因此，论文对于每个像素，进行 text，non-text 和 border 预测，最后通过简单的处理操作就能得到文本框。 MethodArchitecture算法采用自行设计的 FCN，最后输出 text map, non-text map 和 border map。 Figure 2. Fully convolutional networks used in the proposed method. Left: single resolution FCN. Right: multi-resolution FCN.#### Decoder得到最终的概率图后，先根据 text/non-text 得到 connected components，然后使用训练时 border c 的参数放大，得到检测框，具体步骤如下： My Thoughts 论文发现仅依靠 text region 是不足以区分邻近的实例。指出了两个重要问题：why are line-level annotations not sufficient？what can be done to resolve this problem？许多基于分割的方法，都在分析解决这两个问题。 border 确实是个好想法，能把帮助分开不同文本实例，而且后处理操作也简单，同样有篇利用 border 信息的论文[2]。 模型的训练数据是作者们提出的 PPT dataset，但是在 ICDAR 数据上进行训练时，没有讲清楚如何得到 border 标注信息。 另外，需要注意的是，论文里的实验都是基于文本行级别进行的。 References [1] [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning paper code [2] [2017-ICIP] WordFence: Text Detection in Natural Images with Border Awareness paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild]]></title>
    <url>%2F2018%2F12%2F22%2FMulti-scale-FCN-with-Cascaded-Instance-Aware-Segmentation-for-Arbitrary-Oriented-Word-Spotting-In-The-Wild%2F</url>
    <content type="text"><![CDATA[论文基于 FCN 分割的方法，结合 coarse-to-fine 的思想，先得到 text regions，再进行实例级分割得到文本框。 Pipeline 首先，使用 multi-scale FCN 提取图像中的 text regions，对于每个 text region 进行裁剪，依次送进 TL-CNN 中得到其所有的 text center lines，再依次将 text center line 和裁剪的原图合并送进 IA-CNN 进行分割，得到最终的文本框。 ArchitectureMutil-scale FCN 简单来说，就是送进网络训练的图像尺寸不一样，最后把所有的结果进行联合，每个尺寸的损失值也要进行计算，但是不同分支是共用卷积层的。 Multi-FCN architecture Instance segmentation 主要包括 TL-CNN 和 IA-CNN，其方法如下： Method of instance segmentation My Thoughts 想法有点类似于 Faster RCNN 的两阶段处理，只不过这里都是用 FCN 来分割，后来的许多方法是基于 Mask RCNN，其主要思路也是先粗后细。 Multi-scale 的做法，是不是可以在数据预处理进行 scale 的操作来替代，以减少网络训练的复杂度。 论文里提到几个有趣的观察，text block 召回的 false positive，text center line 可以过滤掉。还有就是有些字符的特征很难和背景噪声区分，如“I”，但并不意味字符特征就没用。 References [1] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EAST]]></title>
    <url>%2F2018%2F12%2F19%2FEAST%2F</url>
    <content type="text"><![CDATA[EAST 是一个简洁而高效的文字检测算法，避免了繁琐的后处理方法，直接用 FCN 进行像素级的预测和回归文字框。 ArictectureEAST 使用 PVANet，并借鉴 U-shape 的设计思想，融合高层和底层特征进行预测。Score map 的输出范围为[0,1]，并且代表着基于此像素点预测文本框的置信度。位置信息，可以选择 RBOX 和 QUAD 进行表示。后处理操作包括 Tresholding 和 NMS. Label Generation Score map label 生成如（a）所示，黄色虚线框是人工标注框，绿色框由黄色虚线框边长缩小0.3倍相邻最短边形成的，标注结果如（b）所示。 假设采用 RBOX 标注方式，先生成黄色虚线框的最小外接矩形框（红色），然后计算 score map 中的点分别到四条边的距离，以及红色矩形框的角度。 Limitations论文中，提到了 EAST 的两个缺点： 受限于感受野的大小，不能够很好检测 long text； 对于垂直文本的检测效果不好。 另外，EAST 对于边界的回归也不是很准确。 My Thoughts 针对于感受野小的问题，可以尝试改变卷积核尺寸，宽高比，以及使用 ASPP 等技巧； 对于边界回归不准，可以参考 AdvancedEAST[2]，也可以在训练 geometry 时直接计算点到边的相对距离，这样应该学到更准确的边界信息。 References [1] [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code [2] AdvancedEAST]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scene Text Detection via Holistic, Multi-Channel Prediction]]></title>
    <url>%2F2018%2F12%2F18%2FScene-Text-Detection-via-Holistic-Multi-Channel-Prediction%2F</url>
    <content type="text"><![CDATA[文字区域具有很强的边缘特征，因此论文修改 HED 对图像进行密集预测，输出 text region map, charcter map 以及 linking orientation map，最后融合三个 feature map 生成文本框。 Motivation场景文字检测中，简单进行 text/non-text 分割是不够充分的，有许多文本行距离很近，不能够完全的分开。因此，作者引入更多的监督信息，如字符的位置，大小以及连接方向，最后通过图分割（graph partition）得到文本线（text lines）。 MethodPipeline通过模型得到 text region map，character map，linking orientation map。 Figure 1. Pipeline of the proposed algorithm. (a) Original image. (b) Prediction maps. From left to right: text region map, character map and linking orientation map. (c) Detections. 每个 text region 上的 character 作为顶点，character 之间的相似性作为边，构建图模型，用最大生成求最小割，得到每个文本线。相似性包括空间相似性和方向相似性。空间相似性要求同一个文本实例间的 character 距离相近。方向相似性要求每两个 character 形成的直线方向与 linking orientation map 预测的方向接近。最后，将五个层的预测图经过 1*1 的卷积得到 3 个 maps，跟 ground Truth 计算损失值。 ArchitectureFigure 2. Network architecture of the proposed algorithm 网络的 Stage1~Stage 5 是 VGG16 的前五层，每个 stage 接一个 side-output 输出 3 个 response map，将 stage2~5 后面接 deconvolution 得到原图大小尺寸的预测图。 My Thoughts 论文提出的后处理的方法主要是为了得到文本线，文本线也是可以直接预测的，再结合一些方法，就不用如此复杂的后处理了。 论文中提到下一步可能会预测 character shapes 的二值掩码，能够有利于后续的识别步骤。 References [1] [2016] Scene Text Detection via Holistic, Multi-Channel Prediction paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Accurate text localization in natural image with cascaded convolutional text network]]></title>
    <url>%2F2018%2F12%2F18%2FAccurate-text-localization-in-natural-image-with-cascaded-convolutional-text-network%2F</url>
    <content type="text"><![CDATA[论文采用 coarse-to-fine 的思想，采用两个级联的网络，对于难以区分开不同文本实例的 text region，进行精细识别，得到更准确的结果。 Motivation基于 bottom up 思路的文字检测流程： 用滑动窗口或 MSER/SWT 等方法提取候选区域； 字符级分类器（SVM，CNN等）； 后处理，如文本线生成，字符聚类，字符分组，单词切割等。 然而这种方法存在一些缺点： 使用低级特征，对于光照不均匀，形变较大等目标无法有效提出候选区域； 候选区域很多，分类效率低； 后处理往往比较复杂，规则多，而且不通用； 多步流程容易造成误差累积，导致性能下降。 因此，作者采用 top-down 的思路，直接用 CNN 预测 text region 和 text center linev area。 MethodPipeline算法流程如下： ArchitectureCoarse text network 和 Fine text network 采用 VGG16 的网络结构，并进行了如下修改： 卷积核的尺寸变为多形状，并且是并行的。 去掉全连接层，加入两个卷积层。 多个层的特征进行融合。 Figure 2. Architecture 对于 coarse text network，只用了 text region 的监督信息。而 fine text network 用了 text line area（text region） 和 text center line area的监督信息，两者区别如 Fig3 所示。 Text center line area 的中心线处为1，逐渐向上下扩展，用高斯分布逐渐递减，半径为整个 bbox 高度的 1/4。因此，text center line area 实际上是包含了文本线的位置和文本块的高度信息。 Figure 3. Central line area (middle) and text line area (right) My Thoughts 采用 coarse-to-fine 思想，使用两个网络，参数变多，中间也要设定规则判断是否要输入下个网络，计算流程复杂。 提出的 text center line area 监督信息，使用高斯渐变标注高度信息，很有创新，能解决一些分割不开的情况。 References [1] [2016—CVPR] Accurate text localization in natural image with cascaded convolutional text network paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Multi-Oriented Text Detection with Fully Convolutional Networks]]></title>
    <url>%2F2018%2F12%2F18%2FMulti-Oriented-Text-Detection-with-Fully-Convolutional-Networks%2F</url>
    <content type="text"><![CDATA[检测单个字符容易受背景干扰，造成漏检或误检的情况，而检测文本框相对于单个字符来说，和背景的区分行更强。因此，能不能结合局部信息（单个字符）和上下文信息（文本块）结合起来，使得检测更加鲁棒性。 FCN 在 2015 年被提出去进行像素分割，那么自然地想到能够标定每个像素属于文字的概率（salient map），也可以得到每个像素是字符中心的概率（centroid map）。 Methods算法流程如下图所示，主要分为：Text Block Detection, Text Line Generation 和 Text Line Candidates Classification。 Text Block Detection使用 Text-Block FCN 得到 salient map，然后对其进行过滤，聚类得到文本块。Text-Block FCN 的结构是在 VGG16 的基础上修改的，融合前几层的特征进行预测。 Text Line Generation步骤大体如下： Character Components Extraction：使用 MSER 方法提取字符候选区域，然后利用候选区域的面积和长宽比过滤噪声。 Orientation Estimation：这里假设同一个块中的所有文本线方向是一致的，文本线是近直线的。可以通过寻找最优的垂直坐标偏移 $h$ 和角度 $\theta$ 确定一条一线，使得该直线穿过字符数最多。公式如下，其中 $\Phi$ 是穿过字符的个数。$$\theta_{r}=\arg \max_{\theta} \max_{h} \Phi(\theta ,h)$$ Text Line Candidate Generation: 对每个块里的所有字符进行聚类，然后为每个类生成一个 bounding box。 Text Line Classification通过 Character-Centroid FCN 获得每条文本线中的所有可能存在的字符的中心。然后通过 Intensity criterion 和 Geometric criterion 过滤非文本线。 My Thoughts 这是较早将分割的思想引入到场景文字检测中，但还是用 MSER 方法提取字符，然后聚类，处理过程繁琐，text blocks 只是用来过滤 false positive 的情况。 References [1] [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks paper]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
        <category>Detection</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scene Text Detection and Recognition Paper List]]></title>
    <url>%2F2018%2F12%2F18%2FScene-Text-Detection-and-Recognition-Paper-List%2F</url>
    <content type="text"><![CDATA[Collect and Record some excellent works on scene text detection and recognition. It will keep updating. If you have any suggestions about how to organize these papers, please contact me ! 1. Survey [2018] Scene Text Detection and Recognition: The Deep Learning Era paper [2016-TIP] Text detection, tracking and recognition in video: A comprehensive survey paper [2016-FCS] Scene text detection and recognition: Recent advances and future trends paper [2015-TPMAI] Text detection and recognition in imagery: A survey paper 2. Detection2.1 MethodsConventional [2010-CVPR] Detecting text in natural scenes with stroke width transform paper [2012-CVPR] Detecting Texts of Arbitrary Orientations in Natural Images paper Anchor Based [2018-CVPR] Geometry-Aware Scene Text Detection With Instance Transformation paper code [2018-AAAI] Feature Enhancement Network: A Refined Scene Text Detector paper [2017-AAAI] TextBoxes: A Fast Text Detector with a Single Deep Neural Network paper code [2017-ICCV] Single shot text detector with regional attention paper code [2017-ICCV] Deep direct regression for multi-oriented scene text detection [2017-CVPR] Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection paper [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments paper code [2016-TIP] Text-Attentional Convolutional Neural Networks for Scene Text Detection [2015-CVPR] Symmetry-based text line detection in natural scenes paper Segmentation Based [2018-12] TextField: Learning A Deep Direction Field for Irregular Scene Text Detection paper [2018-11] TextMountain: Accurate Scene Text Detection via Instance Segmentation paper [2018-ECCV] Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping paper [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation papercode [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning paper code [2017-ICIP] WordFence: Text Detection in Natural Images with Border Awareness paper [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild paper [2016] Scene Text Detection via Holistic, Multi-Channel Prediction paper [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks paper [2016-CVPR] Accurate text localization in natural image with cascaded convolutional text network paper Segmentation + Regression [2019-01]MSR: Multi-Scale Shape Regression for Scene Text Detection paper [2018-11] Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks paper [2018-09] TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade paper [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper code [2017] Fused Text Segmentation Networks for Multi-oriented Scene Text Detection paper [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code Attention [2019-AAAI] Scene Text Detection with Supervised Pyramid Context Network paper [2019-WACV] Mask R-CNN with Pyramid Attention Network for Scene Text Detection paper Others [2018-CVPR] Learning Markov Clustering Networks for Scene Text Detection paper [2016-ECCV] Detecting Text in Natural Image with Connectionist Text Proposal Network paper code 2.2 Specific TargetsMulti-Oriented Text [2018-IJCAI] IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection paper code [2018-CVPR] Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation paper [2018-CVPR] Rotation-Sensitive Regression for Oriented Scene Text Detection paper [2017] Arbitrary-oriented scene text detection via rotation proposals paper [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector paper code Irregular Text [2018-12] TextField: Learning A Deep Direction Field for Irregular Scene Text Detection paper [2018-11] TextMountain: Accurate Scene Text Detection via Instance Segmentation paper [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation papercode [2018] Shape Robust Text Detection with Progressive Scale Expansion Network paper [2017] Detecting Curve Text in the Wild: New Dataset and New Solution paper code [2017] Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition paper code Long Text [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes paper [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments paper code 3. Recognition3.1 CTC based methods [2017-TPAMI] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition paper code 3.2 Attention based [2019-01] SAFE: Scale Aware Feature Encoder for Scene Text Recognition paper [2019-AAAI] Scene Text Recognition from Two-Dimensional Perspective paer [2019-11] Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition paper [2018-TPAMI] ASTER: An Attentional Scene Text Recognizer with Flexible Rectification paper code [2018-AAAI] Char-Net: A Character-Aware Neural Network for Distorted Scene Text paper [2018-AAAI] SqueezedText: A Real-time Scene Text Recognition by Binary Convolutional Encoder-decoder Network paper [2018-MM] Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling paper [2017-ICCV] Focusing Attention: Towards Accurate Text Recognition in Natural Images paper [2016-CVPR] Robust Scene Text Recognition with Automatic Rectification paper 3.3 Others [2018-CVPR] Edit Probability for Scene Text Recognition [2018-CVPR] AON: Towards Arbitrarily-Oriented Text Recognition paper 4. End2End Recognition [2018-ECCV] Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes paper code [2018-CVPR] An end-to-end TextSpotter with Explicit Alignment and Attention paper code [2018-CVPR] FOTS: Fast Oriented Text Spotting with a Unified Network paper [2018-AAAI] SEE: Towards Semi-Supervised End-to-End Scene Text Recognition paper [2017-ICCV] Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework paper code [2017-ICCV] Towards end-to-end text spotting with convolutional recurrent neural networks. paper [2018-TIP] TextBoxes++: A Single-Shot Oriented Scene Text Detector. paper code 5. Auxilliary Techs5.1 Synthetic Data [2018-ECCV] Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes paper [2016-CVPR] Synthetic Data for Text Localisation in Natural Images paper 5.2 Bootstrapping [2018-ECCV] Wordsup: Exploiting word annotations for character based text detection [2017-ICCV] Wetext: Scene text detection under weak supervision 5.3 Context Information [2018-ECCV] Using Object Information for Spotting Text paper 5.4 GAN [2018-ECCV] Synthetically Supervised Feature Learning for Scene Text Recognition paper 6. Unsorted [2018-10] Correlation Propagation Networks for Scene Text Detection paper There are also other helpful resources: Awesome-Scene-Text-Recognition OCR Resuourses image-text-localization-recognition SceneTextPapers]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text</tag>
      </tags>
  </entry>
</search>
