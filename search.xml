<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2019 读书计划</title>
    <url>/2019/01/01/2019-%E8%AF%BB%E4%B9%A6%E8%AE%A1%E5%88%92/</url>
    <content><![CDATA[<blockquote class="blockquote-center">You can either travel or read, but either your body or soul must be on the way!   
 Roman Holiday
</blockquote>
<a id="more"></a>
<p><img src="http://static.leiphone.com/uploads/new/article/740_740/201701/588317a4e2934.jpg?imageMogr2/format/jpg/quality/90" alt="image" /></p>
<h4 id="社科类"><a class="markdownIt-Anchor" href="#社科类"></a> 社科类</h4>
<ul>
<li>[ ] 菲利普·津巴多：《态度改变与社会影响》</li>
</ul>
<h4 id="哲学类"><a class="markdownIt-Anchor" href="#哲学类"></a> 哲学类</h4>
<ul>
<li>[ ] 罗伯特·罗素：《西方哲学史》</li>
<li>[ ] 王阳明：《传习录》</li>
<li>[ ] 洪应明：《菜根谭》</li>
<li>[ ] 胡适：《中国哲学史大纲》</li>
</ul>
<h4 id="历史类"><a class="markdownIt-Anchor" href="#历史类"></a> 历史类</h4>
<ul>
<li>[ ] 黄仁宇：《中国大历史》</li>
<li>[ ] 郭建龙：《穿越百年中东》</li>
<li>[ ] 雅克·阿塔利：《未来简史》</li>
<li>[ ] 林汉达：《中国历史故事集》</li>
</ul>
<h4 id="思维类"><a class="markdownIt-Anchor" href="#思维类"></a> 思维类</h4>
<ul>
<li>[x] 查理·芒格：《穷查理宝典》</li>
</ul>
<h4 id="沟通类"><a class="markdownIt-Anchor" href="#沟通类"></a> 沟通类</h4>
<ul>
<li>[ ] 马歇尔·卢森堡：《非暴力沟通》</li>
<li>[ ] 科里帕特森：《关键对话》</li>
</ul>
<h4 id="心理学类"><a class="markdownIt-Anchor" href="#心理学类"></a> 心理学类</h4>
<ul>
<li>[ ] 凯利·麦格尼格尔：《自控力》</li>
<li>[ ] 阿尔弗雷德·阿德勒：《理解人性》</li>
<li>[ ] 戴维·迈尔斯：《社会心理学》</li>
</ul>
<h4 id="小说类"><a class="markdownIt-Anchor" href="#小说类"></a> 小说类</h4>
<ul>
<li>[ ] 阿尔贝.加缪：《局外人》</li>
<li>[ ] 乔治·奥威尔：《1984》</li>
</ul>
<h4 id="其他"><a class="markdownIt-Anchor" href="#其他"></a> 其他</h4>
<ul>
<li>[x] 华杉：《华杉讲透孙子兵法》</li>
</ul>
]]></content>
      <categories>
        <category>读万卷书</category>
      </categories>
      <tags>
        <tag>阅读计划</tag>
      </tags>
  </entry>
  <entry>
    <title>Accurate text localization in natural image with cascaded convolutional text network</title>
    <url>/2018/12/18/Accurate-text-localization-in-natural-image-with-cascaded-convolutional-text-network/</url>
    <content><![CDATA[<p>论文采用 coarse-to-fine 的思想，采用两个级联的网络，对于难以区分开不同文本实例的 text region，进行精细识别，得到更准确的结果。</p>
<a id="more"></a>
<h3 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h3>
<p>基于 bottom up 思路的文字检测流程：</p>
<ul>
<li>用滑动窗口或 MSER/SWT 等方法提取候选区域；</li>
<li>字符级分类器（SVM，CNN等）；</li>
<li>后处理，如文本线生成，字符聚类，字符分组，单词切割等。</li>
</ul>
<p>然而这种方法存在一些缺点：</p>
<ul>
<li>使用低级特征，对于光照不均匀，形变较大等目标无法有效提出候选区域；</li>
<li>候选区域很多，分类效率低；</li>
<li>后处理往往比较复杂，规则多，而且不通用；</li>
<li>多步流程容易造成误差累积，导致性能下降。</li>
</ul>
<p>因此，作者采用 top-down 的思路，直接用 CNN 预测 text region 和 text center linev area。</p>
<h3 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h3>
<h4 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h4>
<p>算法流程如下：</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd66ece153ec.jpg"/></div> 
<h4 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h4>
<p>Coarse text network 和 Fine text network 采用 VGG16 的网络结构，并进行了如下修改：</p>
<ul>
<li>卷积核的尺寸变为多形状，并且是并行的。</li>
<li>去掉全连接层，加入两个卷积层。</li>
<li>多个层的特征进行融合。</li>
<li></li>
</ul>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd66f058e880.jpg"/><p>Figure 2. Architecture</p></div> 
<p>对于 coarse text network，只用了 text region 的监督信息。而 fine text network 用了 text line area（text region） 和 text center line area的监督信息，两者区别如 Fig3 所示。</p>
<p>Text center line area 的中心线处为1，逐渐向上下扩展，用高斯分布逐渐递减，半径为整个 bbox 高度的 1/4。因此，text center line area 实际上是包含了文本线的位置和文本块的高度信息。</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd66f8ddd14b.jpg"/><p>Figure 3. Central line area (middle) and text line area (right)</p></div> 
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>采用 coarse-to-fine 思想，使用两个网络，参数变多，中间也要设定规则判断是否要输入下个网络，计算流程复杂。</li>
<li>提出的 text center line area 监督信息，使用高斯渐变标注高度信息，很有创新，能解决一些分割不开的情况。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2016—CVPR] Accurate text localization in natural image with cascaded convolutional text network <a href="https://arxiv.org/abs/1603.09423"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>2021 读书计划</title>
    <url>/2021/01/01/2021-%E8%AF%BB%E4%B9%A6%E8%AE%A1%E5%88%92/</url>
    <content><![CDATA[<p>读书是百益无害的事情，古今牛人，皆爱读书，难道是他们天资愚钝，才选择读书吗？恰恰相反，正是因为他们聪明，知道读书的好处，才选择手不释卷的。</p>
<a id="more"></a>
<p>读书的目的是站在前人的肩膀上看这个世界，而不是跟随其后，要择其精华，去其糟粕，融会贯通，为我所用。正如陆九渊所说“我注六经，六经注我。”</p>
<p>Keep Reading !</p>
<p><img src="http://static.leiphone.com/uploads/new/article/740_740/201701/588317a4e2934.jpg?imageMogr2/format/jpg/quality/90" alt="image" /></p>
<h4 id="科普类"><a class="markdownIt-Anchor" href="#科普类"></a> 科普类</h4>
<ul>
<li>[ ] 亚当·库哈尔斯基 《传染》</li>
<li>[ ] 吉姆·巴戈特 《物质是什么》</li>
</ul>
<h4 id="商科类"><a class="markdownIt-Anchor" href="#商科类"></a> 商科类</h4>
<ul>
<li>[ ] 威廉·尼克尔斯 《认识商业》</li>
</ul>
<h4 id="社科类"><a class="markdownIt-Anchor" href="#社科类"></a> 社科类</h4>
<ul>
<li>[ ] 迈克尔·罗斯金 《国家的常识》</li>
<li>[ ] 罗伯特.赖特 《洞见》</li>
</ul>
<h4 id="文史类"><a class="markdownIt-Anchor" href="#文史类"></a> 文史类</h4>
<ul>
<li>[ ] 毛泽东 《毛泽东选集》</li>
<li>[ ] 蒋建弄 《毛泽东传》</li>
<li>[ ] 黄晓丹 《诗人十四个》</li>
<li>[ ] 意公子 《大话西方艺术史》</li>
</ul>
]]></content>
      <categories>
        <category>读万卷书</category>
      </categories>
      <tags>
        <tag>阅读计划</tag>
      </tags>
  </entry>
  <entry>
    <title>EAST</title>
    <url>/2018/12/19/EAST/</url>
    <content><![CDATA[<p>EAST 是一个简洁而高效的文字检测算法，避免了繁琐的后处理方法，直接用 FCN 进行像素级的预测和回归文字框。</p>
<a id="more"></a>
<h3 id="arictecture"><a class="markdownIt-Anchor" href="#arictecture"></a> Arictecture</h3>
<p>EAST 使用 PVANet，并借鉴 U-shape 的设计思想，融合高层和底层特征进行预测。Score map 的输出范围为[0,1]，并且代表着基于此像素点预测文本框的置信度。位置信息，可以选择 RBOX 和 QUAD 进行表示。后处理操作包括 Tresholding 和 NMS.</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181219145347.JPG"/></div>
<h3 id="label-generation"><a class="markdownIt-Anchor" href="#label-generation"></a> Label Generation</h3>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181219155513.JPG"/></div>
<p>Score map label 生成如（a）所示，黄色虚线框是人工标注框，绿色框由黄色虚线框边长缩小0.3倍相邻最短边形成的，标注结果如（b）所示。</p>
<p>假设采用 RBOX 标注方式，先生成黄色虚线框的最小外接矩形框（红色），然后计算 score map 中的点分别到四条边的距离，以及红色矩形框的角度。</p>
<h3 id="limitations"><a class="markdownIt-Anchor" href="#limitations"></a> Limitations</h3>
<p>论文中，提到了 EAST 的两个缺点：</p>
<ul>
<li>受限于感受野的大小，不能够很好检测 long text；</li>
<li>对于垂直文本的检测效果不好。</li>
</ul>
<p>另外，EAST 对于边界的回归也不是很准确。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>针对于感受野小的问题，可以尝试改变卷积核尺寸，宽高比，以及使用 ASPP 等技巧；</li>
<li>对于边界回归不准，可以参考 AdvancedEAST[2]，也可以在训练 geometry 时直接计算点到边的相对距离，这样应该学到更准确的边界信息。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector <a href="https://arxiv.org/abs/1704.03155"><code>paper</code></a> <a href="https://github.com/argman/EAST"><code>code</code></a></li>
<li>[2] <a href="https://github.com/huoyijie/AdvancedEAST">AdvancedEAST</a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Learning to Recognize Patch-Wise Consistency for Deepfake Detection</title>
    <url>/2021/01/05/Learning-to-Recognize-Patch-Wise-Consistency-for-Deepfake-Detection/</url>
    <content><![CDATA[<p>主要动机：Deepfake 图像在制作过程中，存在一个拼接融合的操作，伪造区域和非伪造区域来源不同，两者之前可能会存在不同的特征，包括 in-camera featrues (PRNU noise)，out-camera features (JPEG compression patterns, compression retes, frame rates) 和 forgery features (copy-pasete artifacts, blending artifacts, GAN fingerprints)，因此，可以利用伪造区域和邻近区域的特征不一致性来进行鉴伪。</p>
<a id="more"></a>
<p>核心贡献：设计了一个 Patch-wise Consistency Learning (PCL) 分支，给骨干网络提供额外的监督信息，引导模型关注伪造区域和邻近区域的相似性。设计了 Inconsistency Image Generator (I2G) 来生成拼接数据。</p>
<h3 id="patch-wise-consistency-learning-pcl"><a class="markdownIt-Anchor" href="#patch-wise-consistency-learning-pcl"></a> Patch-Wise Consistency Learning (PCL)</h3>
<p>所提方法的总体框架如下所示。主要改动就是添加了一致性比较分支。将骨干网络的中间层分别送入不同的编码器，然后将结果进行点乘，类似注意力机制的操作。这个分支有点类似文献[2]中SPBCU分支，不同的是[2]中是直接预测出伪造区域的分割图，本文是计算所有特征点与其他点的像素，监督信号可能更强一点，不过目的都是想让模型学习到伪造区域与邻近区域的不一致性。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210105104121.PNG"/>
</div>
<h3 id="inconsistency-image-generator-i2g"><a class="markdownIt-Anchor" href="#inconsistency-image-generator-i2g"></a> Inconsistency Image Generator (I2G)</h3>
<p>这个模块和Face-Xray中生成融合伪造图像是一样的，改进很少，严格来讲，不算本文的创新点。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210105105004.PNG"/>
</div>
<h3 id="experiments"><a class="markdownIt-Anchor" href="#experiments"></a> Experiments</h3>
<ul>
<li>表1,2,3做了数据集内部的验证，即训练和测试在同一个数据集上，但是只给出了 PCT+I2G 的结果，没有给出每个模块单独的结果，同时也应该给出只使用骨干网络 Resnet34 的结果；（原因可能是只使用 PCL 效果不好）</li>
<li>表4,5,6做了跨数据集的实验，只使用 I2G 生成伪造数据和真实数据进行训练，证明了模型具有更好的泛化性，另外再DFDC-P上的表现并不突出，作者的解释是图像的质量太低和光线的干扰导致准确率降低；</li>
<li>表7证明了只使用I2G数据效果不够好以及PCL模块的有效性；</li>
<li>表8证明了伪造数据和I2G数据混合训练能够提升模型性能。</li>
</ul>
<h3 id="my-review"><a class="markdownIt-Anchor" href="#my-review"></a> My Review</h3>
<ul>
<li>论文可能很大程度启发于 Face-Xray，只不过后者只考虑拼接边缘，前者考虑所有的区域点；</li>
<li>利用自监督学习范式提升鉴伪模型的泛化性是一个不错的研究小点。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2020-12] Learning to Recognize Patch-Wise Consistency for Deepfake Detection <a href="https://arxiv.org/abs/2012.09311"><code>paper</code></a></li>
<li>[2] [2020-ICME] FSSPOTTER: Spotting Face-Swapped Video by Spatial and Temporal Clues <a href="https://ieeexplore.ieee.org/document/9102914"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Media Forensics</category>
        <category>Face Forgery Detection</category>
      </categories>
      <tags>
        <tag>Face Forgery Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Mask TextSpotter</title>
    <url>/2018/12/25/Mask-TextSpotter/</url>
    <content><![CDATA[<p>论文启发于 Mask RCNN，提出一种 end2end 的场景文字检测和识别模型，能够检测和识别任意形状的文本。</p>
<a id="more"></a>
<p>论文认为文献[2]和[3]虽然提出了 end2end 模型，但是训练的时候还是分步骤训练的，不能完全达到一个 end2end的行为。另外，它们只聚焦于水平或旋转的文本，不能解决弯曲的文本。</p>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<div style="text-align: center">
<img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225125435.JPG"/>
<p>Figure 1. Model architecture</p>
</div>
<p>如 figure 1 所示，mask textspotter 由四部分组成：</p>
<ul>
<li>Backbone：FPN + ResNet50，用来提取特征图。</li>
<li>RPN：生成 text proposals，然后使用 RoI Align 提取 RoI 特征。</li>
<li>Fast RCNN：对 text proposals 进行更精确地分类和回归。</li>
<li>Mask Branch：包含 text instance 分割和 character 分割任务。如 figure 2 所示，会输出一个 global word map，36 个 character maps 和一个 background map。</li>
</ul>
<div style="text-align: center">
<img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225130100.JPG"/>
<p>Figure 2. Mask branch</p>
</div>
<p>character 的 Bbox 被缩小成原来的四分之一。</p>
<h3 id="loss-function"><a class="markdownIt-Anchor" href="#loss-function"></a> Loss Function</h3>
<p>损失函数可以定义为：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><msub><mi>L</mi><mrow><mi>r</mi><mi>p</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi>α</mi><mn>1</mn></msub><msub><mi>L</mi><mrow><mi>r</mi><mi>c</mi><mi>n</mi><mi>n</mi></mrow></msub><mo>+</mo><msub><mi>α</mi><mn>2</mn></msub><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msub><mspace linebreak="newline"></mspace><mspace linebreak="newline"></mspace><msub><mi>L</mi><mrow><mi>m</mi><mi>a</mi><mi>s</mi><mi>k</mi></mrow></msub><mo>=</mo><msub><mi>L</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mi>L</mi><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L = L_{rpn} + \alpha_1 L_{rcnn} + \alpha_2 L_{mask}   \\\\   
L_{mask}=L_{global}+\beta L_{char}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">L</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>这里的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>α</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\alpha_1,\alpha_2,\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span> 都设为 1，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{global}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 是一个平均 binary cross-entropy 损失：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>g</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi></mrow></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo stretchy="false">[</mo><msub><mi>y</mi><mi>n</mi></msub><mo>×</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>×</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">L_{global}= - \frac{1}{N} \sum_{n=1}^{N} [y_n \times \log(S(x_n))+(1-y_n) \times \log(1-S(x_n))]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mopen">[</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p>这里的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 是 sigmoid 函数。</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{char}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是一个加权的 soft-max 损失， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> 对应的 ground truth：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>r</mi></mrow></msub><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>W</mi><mi>n</mi></msub><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover><msub><mi>Y</mi><mrow><mi>n</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mrow><mi>n</mi><mo separator="true">,</mo><mi>t</mi></mrow></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>T</mi><mo>−</mo><mn>1</mn></mrow></munderover><msup><mi>e</mi><msub><mi>x</mi><mrow><mi>n</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub></msup></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{char}= - \frac{1}{N} \sum_{n=1}^{N} W_n \sum_{t=0}^{T-1} Y_{n,t} \log(\frac{e^{x_{n,t}}}{\sum_{k=0}^{T-1}e^{x_{n,k}}})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0954490000000003em;vertical-align:-1.267113em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000002em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000004em;"><span style="top:-1.882887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.128769em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6122219999999999em;"><span style="top:-3.01083em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29011428571428566em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.170941em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
<h3 id="inference"><a class="markdownIt-Anchor" href="#inference"></a> Inference</h3>
<p>测试将 Fast RCNN 产生的 Bbox 经过 NMS 后作为候选框输入 mask branch 生成 global maps 和 character maps。最后产生的 polygons 直接通过计算 global maps 得到。字符序列则通过 pixel voting 算法。</p>
<p>Pixel Voting 流程：</p>
<ul>
<li>将 background map 进行二值化得到所有的字符区域；</li>
<li>对于所有的 character maps 求每个字符区域的像素得分均值，把得分最高的字符类别得分设为该区域输出的字符；</li>
<li>把所有的字符从左到右连接得到字符序列。</li>
</ul>
<p>论文发现如果使用普通的编辑距离去匹配单词，会有多个的编辑距离一样，因此提出 weighted edit distance。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>Mask TextSpotter 本质是用 Faster RCNN 进行目标定位，然后使用目标分割来识别字符，那么可以替换分割算法提升效果。</li>
<li>之前的 text spotter 模型都是先分开训练，再联合训练，这个能直接联合训练。</li>
<li>英文的字母较少，在中文上可能不行。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2018-ECCV] Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes <a href="https://arxiv.org/abs/1807.02242"><code>paper</code></a></li>
<li>[2] [2017-ICCV] Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf"><code>paper</code></a><a href="https://github.com/MichalBusta/DeepTextSpotter"><code>code</code></a></li>
<li>[3] [2017-ICCV] Towards end-to-end text spotting with convolutional recurrent neural networks. <a href="https://arxiv.org/abs/1707.03985"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-Oriented Text Detection with Fully Convolutional Networks</title>
    <url>/2018/12/18/Multi-Oriented-Text-Detection-with-Fully-Convolutional-Networks/</url>
    <content><![CDATA[<p>检测单个字符容易受背景干扰，造成漏检或误检的情况，而检测文本框相对于单个字符来说，和背景的区分行更强。因此，能不能结合局部信息（单个字符）和上下文信息（文本块）结合起来，使得检测更加鲁棒性。</p>
<p>FCN 在 2015 年被提出去进行像素分割，那么自然地想到能够标定每个像素属于文字的概率（salient map），也可以得到每个像素是字符中心的概率（centroid map）。</p>
<a id="more"></a>
<h3 id="methods"><a class="markdownIt-Anchor" href="#methods"></a> Methods</h3>
<p>算法流程如下图所示，主要分为：Text Block Detection, Text Line Generation 和 Text Line Candidates Classification。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181218152913.JPG"/></div>
<h4 id="text-block-detection"><a class="markdownIt-Anchor" href="#text-block-detection"></a> Text Block Detection</h4>
<p>使用 Text-Block FCN 得到 salient map，然后对其进行过滤，聚类得到文本块。Text-Block FCN 的结构是在 VGG16 的基础上修改的，融合前几层的特征进行预测。</p>
<h4 id="text-line-generation"><a class="markdownIt-Anchor" href="#text-line-generation"></a> Text Line Generation</h4>
<p>步骤大体如下：</p>
<ul>
<li>Character Components Extraction：使用 MSER 方法提取字符候选区域，然后利用候选区域的面积和长宽比过滤噪声。</li>
<li>Orientation Estimation：这里假设同一个块中的所有文本线方向是一致的，文本线是近直线的。可以通过寻找最优的垂直坐标偏移 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span></span></span></span> 和角度 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 确定一条一线，使得该直线穿过字符数最多。公式如下，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Φ</span></span></span></span> 是穿过字符的个数。</li>
</ul>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>r</mi></msub><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mo><mi>max</mi><mo>⁡</mo></mo><mi>θ</mi></munder><munder><mo><mi>max</mi><mo>⁡</mo></mo><mi>h</mi></munder><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>θ</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta_{r}=\arg \max_{\theta} \max_{h} \Phi(\theta ,h)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.502108em;vertical-align:-0.752108em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43055999999999994em;"><span style="top:-2.047892em;margin-left:0em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.752108em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43055999999999994em;"><span style="top:-2.047892em;margin-left:0em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.752108em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">Φ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">h</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li>Text Line Candidate Generation: 对每个块里的所有字符进行聚类，然后为每个类生成一个 bounding box。</li>
</ul>
<h4 id="text-line-classification"><a class="markdownIt-Anchor" href="#text-line-classification"></a> Text Line Classification</h4>
<p>通过 Character-Centroid FCN 获得每条文本线中的所有可能存在的字符的中心。然后通过 Intensity criterion 和 Geometric criterion 过滤非文本线。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>这是较早将分割的思想引入到场景文字检测中，但还是用 MSER 方法提取字符，然后聚类，处理过程繁琐，text blocks 只是用来过滤 false positive 的情况。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks <a href="https://arxiv.org/abs/1604.04018"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild</title>
    <url>/2018/12/22/Multi-scale-FCN-with-Cascaded-Instance-Aware-Segmentation-for-Arbitrary-Oriented-Word-Spotting-In-The-Wild/</url>
    <content><![CDATA[<p>论文基于 FCN 分割的方法，结合 coarse-to-fine 的思想，先得到 text regions，再进行实例级分割得到文本框。</p>
<a id="more"></a>
<h3 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h3>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181220170520.JPG"/></div>
<p>首先，使用 multi-scale FCN 提取图像中的 text regions，对于每个 text region 进行裁剪，依次送进 TL-CNN 中得到其所有的 text center lines，再依次将 text center line 和裁剪的原图合并送进 IA-CNN 进行分割，得到最终的文本框。</p>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<p>Mutil-scale FCN 简单来说，就是送进网络训练的图像尺寸不一样，最后把所有的结果进行联合，每个尺寸的损失值也要进行计算，但是不同分支是共用卷积层的。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181220172106.JPG"/><p>Multi-FCN architecture</p></div>
<p>Instance segmentation 主要包括 TL-CNN 和 IA-CNN，其方法如下：</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181220172412.JPG"/><p>Method of instance segmentation</p></div>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>想法有点类似于 Faster RCNN 的两阶段处理，只不过这里都是用 FCN 来分割，后来的许多方法是基于 Mask RCNN，其主要思路也是先粗后细。</li>
<li>Multi-scale 的做法，是不是可以在数据预处理进行 scale 的操作来替代，以减少网络训练的复杂度。</li>
<li>论文里提到几个有趣的观察，text block 召回的 false positive，text center line 可以过滤掉。还有就是有些字符的特征很难和背景噪声区分，如“I”，但并不意味字符特征就没用。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/He_Multi-Scale_FCN_With_CVPR_2017_paper.pdf"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>PixelLink</title>
    <url>/2018/12/22/PixelLink/</url>
    <content><![CDATA[<p>PixelLink 使用实例分割（Instance Segmentation）来实现文本检测，首先分割出文本区域，然后直接找出文本框，完全摒弃位置回归（Location Regression）的思想。</p>
<a id="more"></a>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<p>PixelLink 进行两种像素级别的预测：text/non-text 和 link 预测。标注时，在文本实例内部像素标为 positive，其余的标为 negative。Link 启发于 SegLink，但有所不同，每个像素点有 8 个邻居。给定一个像素点以及它的一个邻居点，如果它们同属于一个文本实例，则它们之间的 link 为 positive。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222170904.JPG"/><p>Figure 1. Structure of PixelLink+VGG16 2s</p></div>
<p>PixelLink 将 VGG-16 结构作为基础网络，将后两层全连接转化为卷积，然后仿照类似 FPN 的结构，融合多层语义信息，最后得到原图二分之一大小的特征图。对于 link 的输出 feature map，其排列的顺序为：top-left，top，top-rignt，left，right，bottom-left，bottom，bottom-right。总的输出维度为 18 。</p>
<p>最后得到 pixels 和 links，然后根据link positive 将pixel positive 进行连接，得到 Cs(conected compoents)集合，集合中的每个元素代表的就是文本实例。两个 pixel 需要连接的前提条件：two link中至少有一个link positive．连接的规则采用的是 Disjoint set data structure (并查集)的方法。</p>
<p>基于上述 CCs 集合，直接掉调用 opencv 的 minAreaRect 提取带方向信息的矩形框。在此之后，还要根据在训练集上统计的信息，进行过滤，去掉噪声。</p>
<h3 id="loss"><a class="markdownIt-Anchor" href="#loss"></a> Loss</h3>
<p>由于文本行的长宽比变化范围广泛，若在计算loss的时候，对所有的 pixel positive 给予相同的权重，这对小面积的文本行是非常不公平的，针对上述问题，论文中提出了Instance-Balanced Cross-Entropy Loss。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>与CTPN，EAST，SegLink相比，PixelLink 对感受野的要求更少，因为每个神经元值只负责预测自己及其邻域内的状态。但是，会不会由于缺乏上下文信息，导致出现假阳性？</li>
<li>PixelLink 不需要用 imagenet 数据集训练的权重进行初始化。</li>
<li>与 SegLink 一样，不能检测很大的文本，这是因为link主要是用于连接相邻的segments，而不能用于检测相距较远的文本行。</li>
<li>后处理部分，严重依赖对数据集统计信息，对实验效果影响占比很大。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation <a href="https://arxiv.org/abs/1801.01315"><code>paper</code></a> <a href="https://github.com/ZJULearning/pixel_link"><code>code</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Scene Text Detection and Recognition Paper List</title>
    <url>/2018/12/18/Scene-Text-Detection-and-Recognition-Paper-List/</url>
    <content><![CDATA[<p>Collect and Record some excellent works on scene text detection and recognition. It will keep updating. If you have any suggestions about how to organize these papers, please contact me !</p>
<a id="more"></a>
<h3 id="1-survey"><a class="markdownIt-Anchor" href="#1-survey"></a> 1. Survey</h3>
<ul>
<li>[x] [2018] Scene Text Detection and Recognition: The Deep Learning Era <a href="https://arxiv.org/abs/1811.04256"><code>paper</code></a></li>
<li>[ ] [2016-TIP] Text detection, tracking and recognition in video: A comprehensive survey <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7452620"><code>paper</code></a></li>
<li>[x] [2016-FCS] Scene text detection and recognition: Recent advances and future trends <a href="https://www.researchgate.net/profile/Xiang_Bai4/publication/286945604_Scene_text_detection_and_recognition_recent_advances_and_future_trends/links/57f7212b08ae280dd0bb3d6b/Scene-text-detection-and-recognition-recent-advances-and-future-trends.pdf?origin=publication_detail"><code>paper</code></a></li>
<li>[x] [2015-TPMAI] Text detection and recognition in imagery: A survey <a href="https://ucassdl.cn/downloads/publication/PAMI2015_YeQixiang.pdf"><code>paper</code></a></li>
</ul>
<h3 id="2-detection"><a class="markdownIt-Anchor" href="#2-detection"></a> 2. Detection</h3>
<h4 id="21-methods"><a class="markdownIt-Anchor" href="#21-methods"></a> 2.1 Methods</h4>
<h5 id="conventional"><a class="markdownIt-Anchor" href="#conventional"></a> Conventional</h5>
<ul>
<li>[ ] [2010-CVPR] Detecting text in natural scenes with stroke width transform <a href="http://www.math.tau.ac.il/~turkel/imagepapers/text_detection.pdf"><code>paper</code></a></li>
<li>[ ] [2012-CVPR] Detecting Texts of Arbitrary Orientations in Natural Images <a href="http://pages.ucsd.edu/~ztu/publication/cvpr12_textdetection.pdf"><code>paper</code></a></li>
</ul>
<h5 id="anchor-based"><a class="markdownIt-Anchor" href="#anchor-based"></a> Anchor Based</h5>
<ul>
<li>[x] [2018-CVPR] Geometry-Aware Scene Text Detection With Instance Transformation <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1653.pdf"><code>paper</code></a> <a href="https://github.com/zlmzju/itn"><code>code</code></a></li>
<li>[ ] [2018-AAAI] Feature Enhancement Network: A Refined Scene Text Detector <a href="https://arxiv.org/pdf/1711.04249.pdf"><code>paper</code></a></li>
<li>[ ] [2017-AAAI] TextBoxes: A Fast Text Detector with a Single Deep Neural Network <a href="https://arxiv.org/abs/1611.06779"><code>paper</code></a> <a href="https://github.com/MhLiao/TextBoxes"><code>code</code></a></li>
<li>[ ] [2017-ICCV] Single shot text detector with regional attention <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Single_Shot_Text_ICCV_2017_paper.pdf"><code>paper</code></a> <a href="https://github.com/HotaekHan/SSTDNet"><code>code</code></a></li>
<li>[ ] [2017-ICCV] Deep direct regression for multi-oriented scene text detection</li>
<li>[ ] [2017-CVPR] Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection <a href="https://arxiv.org/abs/1703.01425"><code>paper</code></a></li>
<li>[x] [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments <a href="https://arxiv.org/abs/1703.06520"><code>paper</code></a> <a href="https://github.com/bgshih/seglink"><code>code</code></a></li>
<li>[ ] [2016-TIP] Text-Attentional Convolutional Neural Networks for Scene Text Detection</li>
<li>[ ] [2015-CVPR] Symmetry-based text line detection in natural scenes <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper.pdf"><code>paper</code></a></li>
</ul>
<h5 id="segmentation-based"><a class="markdownIt-Anchor" href="#segmentation-based"></a> Segmentation Based</h5>
<ul>
<li>[x] [2018-12] TextField: Learning A Deep Direction Field for Irregular Scene Text Detection <a href="https://arxiv.org/abs/1812.01393"><code>paper</code></a></li>
<li>[x] [2018-11] TextMountain: Accurate Scene Text Detection via Instance Segmentation <a href="https://arxiv.org/abs/1811.12786"><code>paper</code></a></li>
<li>[x] [2018-ECCV] Accurate Scene Text Detection through Border Semantics Awareness and Bootstrapping <a href="https://arxiv.org/abs/1807.03547"><code>paper</code></a></li>
<li>[x] [2018] Shape Robust Text Detection with Progressive Scale Expansion Network <a href="http://export.arxiv.org/abs/1806.02559"><code>paper</code></a></li>
<li>[x] [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation <a href="https://arxiv.org/abs/1801.01315"><code>paper</code></a><a href="https://github.com/ZJULearning/pixel_link"><code>code</code></a></li>
<li>[x] [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_Self-Organized_Text_Detection_ICCV_2017_paper.pdf"><code>paper</code></a> <a href="https://github.com/isi-vista/textDetectionWithScriptID"><code>code</code></a></li>
<li>[x] [2017-ICIP] WordFence: Text Detection in Natural Images with Border Awareness <a href="https://arxiv.org/abs/1705.05483"><code>paper</code></a></li>
<li>[x] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/He_Multi-Scale_FCN_With_CVPR_2017_paper.pdf"><code>paper</code></a></li>
<li>[x] [2016] Scene Text Detection via Holistic, Multi-Channel Prediction <a href="https://arxiv.org/abs/1606.09002"><code>paper</code></a></li>
<li>[x] [2016-CVPR] Multi-Oriented Text Detection with Fully Convolutional Networks <a href="https://arxiv.org/abs/1604.04018"><code>paper</code></a></li>
<li>[x] [2016-CVPR] Accurate text localization in natural image with cascaded convolutional text network <a href="https://arxiv.org/abs/1603.09423"><code>paper</code></a></li>
</ul>
<h5 id="segmentation-regression"><a class="markdownIt-Anchor" href="#segmentation-regression"></a> Segmentation + Regression</h5>
<ul>
<li>[ ] [2019-01]MSR: Multi-Scale Shape Regression for Scene Text Detection <a href="https://arxiv.org/abs/1901.02596"><code>paper</code></a></li>
<li>[x] [2018-11] Pixel-Anchor: A Fast Oriented Scene Text Detector with Combined Networks <a href="https://arxiv.org/abs/1811.07432"><code>paper</code></a></li>
<li>[x] [2018-09] TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade <a href="https://arxiv.org/abs/1809.03050"><code>paper</code></a></li>
<li>[x] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes <a href="https://arxiv.org/pdf/1807.01544.pdf"><code>paper</code></a> <a href="https://github.com/princewang1994/TextSnake.pytorch"><code>code</code></a></li>
<li>[x] [2017] Fused Text Segmentation Networks for Multi-oriented Scene Text Detection <a href="https://arxiv.org/abs/1709.03272"><code>paper</code></a></li>
<li>[x] [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector <a href="https://arxiv.org/abs/1704.03155"><code>paper</code></a> <a href="https://github.com/argman/EAST"><code>code</code></a></li>
</ul>
<h5 id="attention"><a class="markdownIt-Anchor" href="#attention"></a> Attention</h5>
<ul>
<li>[x] [2019-AAAI] Scene Text Detection with Supervised Pyramid Context Network <a href="https://arxiv.org/abs/1811.08605"><code>paper</code></a></li>
<li>[x] [2019-WACV] Mask R-CNN with Pyramid Attention Network for Scene Text Detection <a href="https://arxiv.org/abs/1811.09058"><code>paper</code></a></li>
</ul>
<h5 id="others"><a class="markdownIt-Anchor" href="#others"></a> Others</h5>
<ul>
<li>[x] [2018-CVPR] Learning Markov Clustering Networks for Scene Text Detection <a href="https://arxiv.org/abs/1805.08365"><code>paper</code></a></li>
<li>[x] [2016-ECCV] Detecting Text in Natural Image with Connectionist Text Proposal Network <a href="https://arxiv.org/abs/1609.03605"><code>paper</code></a> <a href="https://github.com/eragonruan/text-detection-ctpn"><code>code</code></a></li>
</ul>
<h4 id="22-specific-targets"><a class="markdownIt-Anchor" href="#22-specific-targets"></a> 2.2 Specific Targets</h4>
<h5 id="multi-oriented-text"><a class="markdownIt-Anchor" href="#multi-oriented-text"></a> Multi-Oriented Text</h5>
<ul>
<li>[x] [2018-IJCAI] IncepText: A New Inception-Text Module with Deformable PSROI Pooling for Multi-Oriented Scene Text Detection <a href="https://arxiv.org/pdf/1805.01167.pdf"><code>paper</code></a> <a href="https://github.com/xieyufei1993/InceptText-Tensorflow"><code>code</code></a></li>
<li>[x] [2018-CVPR] Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation <a href="https://arxiv.org/abs/1802.08948"><code>paper</code></a></li>
<li>[x] [2018-CVPR] Rotation-Sensitive Regression for Oriented Scene Text Detection <a href="https://arxiv.org/abs/1803.05265"><code>paper</code></a></li>
<li>[ ] [2017] Arbitrary-oriented scene text detection via rotation proposals <a href="https://arxiv.org/abs/1703.01086"><code>paper</code></a></li>
<li>[x] [2017-CVPR] EAST: An Efficient and Accurate Scene Text Detector <a href="https://arxiv.org/abs/1704.03155"><code>paper</code></a> <a href="https://github.com/argman/EAST"><code>code</code></a></li>
</ul>
<h5 id="irregular-text"><a class="markdownIt-Anchor" href="#irregular-text"></a> Irregular Text</h5>
<ul>
<li>[x] [2018-12] TextField: Learning A Deep Direction Field for Irregular Scene Text Detection <a href="https://arxiv.org/abs/1812.01393"><code>paper</code></a></li>
<li>[x] [2018-11] TextMountain: Accurate Scene Text Detection via Instance Segmentation <a href="https://arxiv.org/abs/1811.12786"><code>paper</code></a></li>
<li>[x] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes <a href="https://arxiv.org/pdf/1807.01544.pdf"><code>paper</code></a></li>
<li>[x] [2018-AAAI] PixelLink: Detecting Scene Text via Instance Segmentation <a href="https://arxiv.org/abs/1801.01315"><code>paper</code></a><a href="https://github.com/ZJULearning/pixel_link"><code>code</code></a></li>
<li>[x] [2018] Shape Robust Text Detection with Progressive Scale Expansion Network <a href="http://export.arxiv.org/abs/1806.02559"><code>paper</code></a></li>
<li>[x] [2017] Detecting Curve Text in the Wild: New Dataset and New Solution <a href="https://arxiv.org/abs/1712.02170"><code>paper</code></a> <a href="https://github.com/Yuliang-Liu/Curve-Text-Detector"><code>code</code></a></li>
<li>[x] [2017] Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition <a href="https://arxiv.org/abs/1710.10400"><code>paper</code></a> <a href="https://github.com/cs-chan/Total-Text-Dataset"><code>code</code></a></li>
</ul>
<h5 id="long-text"><a class="markdownIt-Anchor" href="#long-text"></a> Long Text</h5>
<ul>
<li>[x] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes <a href="https://arxiv.org/pdf/1807.01544.pdf"><code>paper</code></a></li>
<li>[x] [2017-CVPR] Detecting Oriented Text in Natural Images by Linking Segments <a href="https://arxiv.org/abs/1703.06520"><code>paper</code></a> <a href="https://github.com/bgshih/seglink"><code>code</code></a></li>
</ul>
<h3 id="3-recognition"><a class="markdownIt-Anchor" href="#3-recognition"></a> 3. Recognition</h3>
<h4 id="31-ctc-based-methods"><a class="markdownIt-Anchor" href="#31-ctc-based-methods"></a> 3.1 CTC based methods</h4>
<ul>
<li>[x] [2017-TPAMI] An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition <a href="https://arxiv.org/abs/1507.05717"><code>paper</code></a> <a href="https://github.com/bgshih/crnn"><code>code</code></a></li>
</ul>
<h4 id="32-attention-based"><a class="markdownIt-Anchor" href="#32-attention-based"></a> 3.2 Attention based</h4>
<ul>
<li>[x] [2019-01] SAFE: Scale Aware Feature Encoder for Scene Text Recognition <a href="https://arxiv.org/abs/1901.05770"><code>paper</code></a></li>
<li>[x] [2019-AAAI]  Scene Text Recognition from Two-Dimensional Perspective <a href="https://arxiv.org/abs/1809.06508"><code>paer</code></a></li>
<li>[x] [2019-11] Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition <a href="https://arxiv.org/abs/1811.00751"><code>paper</code></a></li>
<li>[x] [2018-TPAMI] ASTER: An Attentional Scene Text Recognizer with Flexible Rectification <a href="http://cloud.eic.hust.edu.cn:8071/UpLoadFiles/Papers/ASTER_PAMI18.pdf"><code>paper</code></a> <a href="https://github.com/bgshih/aster"><code>code</code></a></li>
<li>[x] [2018-AAAI] Char-Net: A Character-Aware Neural Network for Distorted Scene Text <a href="http://www.visionlab.cs.hku.hk/publications/wliu_aaai18.pdf"><code>paper</code></a></li>
<li>[ ] [2018-AAAI] SqueezedText: A Real-time Scene Text Recognition by Binary Convolutional Encoder-decoder Network <a href="https://pdfs.semanticscholar.org/9061/47e6eb8e963d9751dda18fb540ed7faeb9fb.pdf"><code>paper</code></a></li>
<li>[ ] [2018-MM] Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling <a href="https://arxiv.org/pdf/1709.02054.pdf"><code>paper</code></a></li>
<li>[ ] [2017-ICCV] Focusing Attention: Towards Accurate Text Recognition in Natural Images <a href="https://arxiv.org/pdf/1709.02054.pdf"><code>paper</code></a></li>
<li>[x] [2016-CVPR] Robust Scene Text Recognition with Automatic Rectification <a href="https://arxiv.org/abs/1603.03915"><code>paper</code></a></li>
</ul>
<h4 id="33-others"><a class="markdownIt-Anchor" href="#33-others"></a> 3.3 Others</h4>
<ul>
<li>[ ] [2018-CVPR] Edit Probability for Scene Text Recognition</li>
<li>[ ] [2018-CVPR] AON: Towards Arbitrarily-Oriented Text Recognition <a href="https://arxiv.org/abs/1711.04226"><code>paper</code></a></li>
</ul>
<h3 id="4-end2end-recognition"><a class="markdownIt-Anchor" href="#4-end2end-recognition"></a> 4. End2End Recognition</h3>
<ul>
<li>[x] [2018-ECCV] Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes <a href="https://arxiv.org/abs/1807.02242"><code>paper</code></a> <a href="https://github.com/lvpengyuan/masktextspotter.caffe2"><code>code</code></a></li>
<li>[x] [2018-CVPR] An end-to-end TextSpotter with Explicit Alignment and Attention <a href="https://arxiv.org/abs/1803.03474"><code>paper</code></a> <a href="https://github.com/tonghe90/textspotter"><code>code</code></a></li>
<li>[x] [2018-CVPR] FOTS: Fast Oriented Text Spotting with a Unified Network <a href="https://arxiv.org/pdf/1801.01671.pdf"><code>paper</code></a></li>
<li>[ ] [2018-AAAI] SEE: Towards Semi-Supervised End-to-End Scene Text Recognition <a href="https://arxiv.org/pdf/1712.05404.pdf"><code>paper</code></a></li>
<li>[x] [2017-ICCV] Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Busta_Deep_TextSpotter_An_ICCV_2017_paper.pdf"><code>paper</code></a> <a href="https://github.com/MichalBusta/DeepTextSpotter"><code>code</code></a></li>
<li>[x] [2017-ICCV] Towards end-to-end text spotting with convolutional recurrent neural networks. <a href="https://arxiv.org/abs/1707.03985"><code>paper</code></a></li>
<li>[ ] [2018-TIP] TextBoxes++: A Single-Shot Oriented Scene Text Detector. <a href="https://arxiv.org/abs/1801.02765"><code>paper</code></a> <a href="https://github.com/MhLiao/TextBoxes_plusplus"><code>code</code></a></li>
</ul>
<h3 id="5-auxilliary-techs"><a class="markdownIt-Anchor" href="#5-auxilliary-techs"></a> 5. Auxilliary Techs</h3>
<h4 id="51-synthetic-data"><a class="markdownIt-Anchor" href="#51-synthetic-data"></a> 5.1 Synthetic Data</h4>
<ul>
<li>[ ] [2018-ECCV] Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fangneng_Zhan_Verisimilar_Image_Synthesis_ECCV_2018_paper.pdf"><code>paper</code></a></li>
<li>[x] [2016-CVPR] Synthetic Data for Text Localisation in Natural Images  <a href="http://www.robots.ox.ac.uk/~ankush/textloc.pdf"><code>paper</code></a></li>
</ul>
<h4 id="52-bootstrapping"><a class="markdownIt-Anchor" href="#52-bootstrapping"></a> 5.2 Bootstrapping</h4>
<ul>
<li>[ ] [2018-ECCV] Wordsup: Exploiting word annotations for character based text detection</li>
<li>[ ] [2017-ICCV] Wetext: Scene text detection under weak supervision</li>
</ul>
<h4 id="53-context-information"><a class="markdownIt-Anchor" href="#53-context-information"></a> 5.3 Context Information</h4>
<ul>
<li>[x] [2018-ECCV] Using Object Information for Spotting Text <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Shitala_Prasad_Using_Object_Information_ECCV_2018_paper.pdf"><code>paper</code></a></li>
</ul>
<h4 id="54-gan"><a class="markdownIt-Anchor" href="#54-gan"></a> 5.4 GAN</h4>
<ul>
<li>[x] [2018-ECCV] Synthetically Supervised Feature Learning for Scene Text Recognition <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Liu_Synthetically_Supervised_Feature_ECCV_2018_paper.pdf"><code>paper</code></a></li>
</ul>
<h3 id="6-unsorted"><a class="markdownIt-Anchor" href="#6-unsorted"></a> 6. Unsorted</h3>
<ul>
<li>[ ] [2018-10] Correlation Propagation Networks for Scene Text Detection <a href="https://arxiv.org/abs/1810.00304"><code>paper</code></a></li>
</ul>
<p>There are also other helpful resources:</p>
<ul>
<li><a href="https://github.com/chongyangtao/Awesome-Scene-Text-Recognition">Awesome-Scene-Text-Recognition</a></li>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/ocr.html">OCR Resuourses</a></li>
<li><a href="https://github.com/whitelok/image-text-localization-recognition/blob/master/README.zh-cn.md">image-text-localization-recognition</a></li>
<li><a href="https://github.com/Jyouhou/SceneTextPapers">SceneTextPapers</a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text</tag>
      </tags>
  </entry>
  <entry>
    <title>Scene Text Detection via Holistic, Multi-Channel Prediction</title>
    <url>/2018/12/18/Scene-Text-Detection-via-Holistic-Multi-Channel-Prediction/</url>
    <content><![CDATA[<p>文字区域具有很强的边缘特征，因此论文修改 HED 对图像进行密集预测，输出 text region map, charcter map 以及 linking orientation map，最后融合三个 feature map 生成文本框。</p>
<a id="more"></a>
<h3 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h3>
<p>场景文字检测中，简单进行 text/non-text 分割是不够充分的，有许多文本行距离很近，不能够完全的分开。因此，作者引入更多的监督信息，如字符的位置，大小以及连接方向，最后通过图分割（graph partition）得到文本线（text lines）。</p>
<h3 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h3>
<h4 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h4>
<p>通过模型得到 text region map，character map，linking orientation map。</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd6634207b33.jpg"/><p>Figure 1. Pipeline of the proposed algorithm. (a) Original image. (b) Prediction maps. From left to right: text region map, character map and linking orientation map.  (c) Detections.</p>
</div>
<p>每个 text region 上的 character 作为顶点，character 之间的相似性作为边，构建图模型，用最大生成求最小割，得到每个文本线。相似性包括空间相似性和方向相似性。空间相似性要求同一个文本实例间的 character 距离相近。方向相似性要求每两个 character 形成的直线方向与 linking orientation map 预测的方向接近。最后，将五个层的预测图经过 1*1 的卷积得到 3 个 maps，跟 ground Truth 计算损失值。</p>
<h4 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h4>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd664df87c6f.jpg"/><p>Figure 2. Network architecture of the proposed algorithm</p></div>
<p>网络的 Stage1~Stage 5 是 VGG16 的前五层，每个 stage 接一个 side-output 输出 3 个 response map，将 stage2~5 后面接 deconvolution 得到原图大小尺寸的预测图。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>论文提出的后处理的方法主要是为了得到文本线，文本线也是可以直接预测的，再结合一些方法，就不用如此复杂的后处理了。</li>
<li>论文中提到下一步可能会预测 character shapes 的二值掩码，能够有利于后续的识别步骤。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2016] Scene Text Detection via Holistic, Multi-Channel Prediction <a href="https://arxiv.org/abs/1606.09002"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-organized Text Detection with Minimal Post-processing via Border Learning</title>
    <url>/2018/12/22/Self-organized-Text-Detection-with-Minimal-Post-processing-via-Border-Learning/</url>
    <content><![CDATA[<p>论文提出了基于分割的文本行级别的检测算法，通过引入 text border，再经过极少的后处理操作就能得到文本框。</p>
<a id="more"></a>
<h3 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h3>
<p>根据检测单元的不同，相关算法可以分为：</p>
<ul>
<li>component detection：检测字符的一部分，再通过后处理连成检测框。</li>
<li>character detection：字符是单词的最小组成单元。</li>
<li>word detection：单词是常用的标准。</li>
<li>line detection：符合人类的阅读习惯。</li>
<li>region detection：能够排除很多 fasle positive。</li>
</ul>
<p>其中，component/character 检测可划分为 bottom-up 的方法，而 region 检测为 up-down 的方法。通常，word/line 层次的检测框是想要的最终的结果，那么，直接把候选框就定为 word/line 级别则更方便。另外，现在许多基于分割的方法，后处理的操作过于复杂，且不是端到端的训练。因此，论文对于每个像素，进行 text，non-text 和 border 预测，最后通过简单的处理操作就能得到文本框。</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd6706cd4638.jpg"/></div>
<h3 id="method"><a class="markdownIt-Anchor" href="#method"></a> Method</h3>
<h4 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h4>
<p>算法采用自行设计的 FCN，最后输出 text map, non-text map 和 border map。</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd670e55fa95.jpg"/><p>Figure 2. Fully convolutional networks used in the proposed method. Left: single resolution FCN. Right: multi-resolution FCN.</p></div>
#### Decoder
<p>得到最终的概率图后，先根据 text/non-text 得到 connected components，然后使用训练时 border c 的参数放大，得到检测框，具体步骤如下：</p>
<div style="text-align: center"><img src="https://i.loli.net/2018/10/29/5bd6713b2c0a5.jpg"/></div>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>论文发现仅依靠 text region 是不足以区分邻近的实例。指出了两个重要问题：why are line-level annotations not sufficient？what can be done to resolve this problem？许多基于分割的方法，都在分析解决这两个问题。</li>
<li>border 确实是个好想法，能把帮助分开不同文本实例，而且后处理操作也简单，同样有篇利用 border 信息的论文[2]。</li>
<li>模型的训练数据是作者们提出的 PPT dataset，但是在 ICDAR 数据上进行训练时，没有讲清楚如何得到 border 标注信息。</li>
<li>另外，需要注意的是，论文里的实验都是基于文本行级别进行的。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2017-ICCV] Self-organized Text Detection with Minimal Post-processing via Border Learning <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_Self-Organized_Text_Detection_ICCV_2017_paper.pdf"><code>paper</code></a> <a href="https://github.com/isi-vista/textDetectionWithScriptID"><code>code</code></a></li>
<li>[2] [2017-ICIP] WordFence: Text Detection in Natural Images with Border Awareness <a href="https://arxiv.org/abs/1705.05483"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>Shape Robust Text Detection with Progressive Scale Expansion Network</title>
    <url>/2018/12/22/Shape-Robust-Text-Detection-with-Progressive-Scale-Expansion-Network/</url>
    <content><![CDATA[<p>针对任意形状文字检测，论文提出Progressive Scale Expansion Network (PSENet)，通过预测不同尺寸的文字区域的 kernels，然后采用 Breadth-First-Search 的方法从最小尺寸的 kernel 进行逐渐扩张到最大尺寸的 kernel。</p>
<a id="more"></a>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<p>PSENet 使用 Resnet 作为 backbone，结合 FPN，将不同层次的特征上采样到同样大小，然后拼接起来，预测不同尺寸的 kernels。最后通过 Progressive Scale Expansion 算法去不断地扩大 kernel，从而得到每个文字块。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222155324.JPG"/><p>PSENet 整体结构</p></div>
<h3 id="progressive-scale-expansion-algorithm"><a class="markdownIt-Anchor" href="#progressive-scale-expansion-algorithm"></a> Progressive Scale Expansion Algorithm</h3>
<p>PSE 主要基于宽度优先搜索算法，先通过最小的 kernel 分割图，得到 connected components，此时，由于 kernel 较小，得到的是文本块的中心区域，能够将邻近的文本实例区分开，但是边界信息不准确。因此，将 CCs 中的元素压如队列，再依次和大尺寸的 kernel 分割结果合并。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222162939.JPG"/></div>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>基于分割的方法，对于邻近的文本实例不容易分开，逐渐扩充的想法，简单而有效。</li>
<li>论文中生成 label 的方法可以参考借鉴。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2018] Shape Robust Text Detection with Progressive Scale Expansion Network <a href="http://export.arxiv.org/abs/1806.02559"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>TextContourNet</title>
    <url>/2018/12/25/TextContourNet/</url>
    <content><![CDATA[<p>TextContourNet 利用实例级别的场景文字的边界轮廓信息，帮助提高文字检测器的能力。</p>
<a id="more"></a>
<h3 id="text-contour"><a class="markdownIt-Anchor" href="#text-contour"></a> Text Contour</h3>
<p>这里的文本轮廓不是语义级别的，是针对每个文本实例。轮廓可以由边界框来生成，论文中使用一种平滑的轮廓信息来进行训练，其生成公式如下所示，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>o</mi><mi>u</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">S_{contour}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示边界框上的像素点集合。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225094959.JPG"/></div>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<p>论文主要想论证，引入轮廓信息能够帮助常规的文本检测器提高检测效果。选用 EAST 模型作为基础文本检测器。并设计了几种不同的引入方法。</p>
<p>Auxiliary TextContourNet，将 contour task 作为 auxiliary loss。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225095618.JPG"/></div>
<p>Cascade TextContourNet 设计两种方案。第一种，两个分支不共享 encoder 模块，将得到的轮廓结果和原图拼接在一起，再进行检测。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225100159.JPG"/></div>
<p>第二种就是共享 encoder 模块，将得到的轮廓结果和检测分支的特征拼接在一起，再进行检测。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181225100656.JPG"/></div>
<p>最后的实验结果表明，Cascade 模式下共享 encoder 的效果是最好的，可能的原因是在两种损失的联合下，encoder 部分训练得更好，可以再补充一个实验证明一下，即共享 encoder 模块，将得到的轮廓结果和原图拼接，进行检测。</p>
<h3 id="my-thoughts"><a class="markdownIt-Anchor" href="#my-thoughts"></a> My Thoughts</h3>
<ul>
<li>论文中提到的 Text Contour 其实和 Text Border 的概念是类似，早有工作将 border 信息引入场景文字检测中，确实发现有效果。</li>
<li>contour ground truth 采用平滑的方式，想法不错。</li>
<li>利用分割得到一些辅助信息，然后和检测分支拼接，帮助提高检测效果。有许多工作进行尝试，如文献[3]中，通过分割得到 text/non-text map，来消除背景噪声。文献[2]提取 text center line 再和原图拼接，进行检测，不过它是 crop 每个 text regions 进行的，比较繁琐。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2018]TextContourNet: a Flexible and Effective Framework for Improving Scene Text Detection Architecture with a Multi-task Cascade <a href="https://arxiv.org/abs/1809.03050"><code>paper</code></a></li>
<li>[2] [2017-CVPR] Multi-scale FCN with Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting In The Wild <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/He_Multi-Scale_FCN_With_CVPR_2017_paper.pdf"><code>paper</code></a></li>
<li>[3] [2017-ICCV] Single shot text detector with regional attention <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Single_Shot_Text_ICCV_2017_paper.pdf"><code>paper</code></a> <a href="https://github.com/HotaekHan/SSTDNet"><code>code</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>TextSnake</title>
    <url>/2018/12/22/TextSnake/</url>
    <content><![CDATA[<p>TextSnake，是一个用于检测任意形状文本的灵活表征。</p>
<a id="more"></a>
<h3 id="motivation"><a class="markdownIt-Anchor" href="#motivation"></a> Motivation</h3>
<p>现有的方法中，有一个共同的假设，文本行实例的形状大体上是线性的，因此，可以采用相对简单的表征方式进行描述，如 Fig 1 中（a）轴对齐矩形,（b）旋转矩形，（c）四边形。然而，在面对带有透视形变（perspective distortion）的<br />
弧形文字 （curved text），这些表征方法在精确估计几何属性方面会有所欠缺，而且会引入更多的背景无关区域。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222191931.JPG"/><p>Figure 1. Comparison of different representations for text instances</p></div>
<p>因此，作者提出 TextSnake，可以适应各种形状的文本实例。本质思想使用凸 N 边形去包裹文本实例，论文结合极坐标很好的实现出来了。</p>
<h3 id="mehthod"><a class="markdownIt-Anchor" href="#mehthod"></a> Mehthod</h3>
<h4 id="representation"><a class="markdownIt-Anchor" href="#representation"></a> Representation</h4>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222200442.JPG"/><p>Figure 2. TextSnake representation</p></div>
TextSnake 将一个文本区域（黄色）表征为一系列有序而重叠的圆盘（蓝色），其中每个圆盘都由一条中心线（绿色，即对称轴或骨架）贯穿，并带有可变的半径 r 和方向 θ 。直观讲，TextSnake 能够改变其形状以适应不同的变化，比如旋转，缩放，弯曲。
<p>圆盘并非一一对应于文本实例的字符。但是圆盘序列的几何属性可以改正不规则形状的文本实例，并将其转化为对文本识别器更加友好的矩形等。</p>
<h4 id="pipeline"><a class="markdownIt-Anchor" href="#pipeline"></a> Pipeline</h4>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222200521.JPG"/><p>Figure 3. Method framework: network output and post-processing</p></div>
<p>为检测任意形状的文本，本文借助 FCN 模型预测文本实例的几何属性。基于 FCN 的网络预测文本中心线（TCL），文本区域（TR）以及几何属性（包括 r，cosθ，sinθ）的分值图。由于 TCL 是 TR 的一部分，通过 TR 而得到 Masked TCL。假定 TCL 没有彼此重合，需要借助并查集（disjoint set）执行实例分割。Striding Algorithm 用于提取中心轴点，并最终重建文本实例。</p>
<h4 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h4>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222200559.JPG"/><p>Figure 4. Network Architecture. Blue blocks are convolution stages of VGG-16</p></div>
在 FPN 和 U-net 的启发下，本文提出一个方案，可逐渐融合来自主干网络不同层级的特征。主干网络可以是用于图像分类的卷积网络，比如 VGG-16/19 和 ResNet。这些网络可以被分成 5 个卷积阶段（stage）和若干个额外的全连接层。本文移除全连接层，并在每个阶段之后将特征图馈送至特征融合网络。
<h4 id="inference"><a class="markdownIt-Anchor" href="#inference"></a> Inference</h4>
<p>经过前向传播之后，网络输出 TCL，TR 以及几何图。对于 TCL 和 TR，阈值分别设为 T_tcl 和 T_tr；接着，TCL 和 TR 的交叉点给出 TCL 最后的预测。通过并查集，可以有效把 TCL 像素分割进不同的文本实例。最后，Striding Algorithm 被设计以提取用来表示文本实例形状和进程（course）的有序点，同时重建文本实例区域。</p>
<div style="text-align: center"><img src="https://raw.githubusercontent.com/LightningChan/pictures/master/2018/20181222200635.JPG"/><p>Figure 5. Framework of Post-processing Algorithm</p></div>
<p>Striding Algorithm 的流程如 Fig 5 所示。它主要包含 3 个部分：Act(a)Centralizing ，Act(b) Striding 和 Act©Sliding 。首先，本文随机选择一个像素作为起点，并将其中心化。接着，搜索过程分支为两个相反的方向——striding 和 centralizing 直到结束。这一过程将在两个相反方向上生成两个有序点，并可结合以生成最终的中心轴，它符合文本的进程，并精确描述形状。</p>
<h3 id="experiments"><a class="markdownIt-Anchor" href="#experiments"></a> Experiments</h3>
<p>Total-Text &amp; CTW1500 数据集上展开的是有关曲形文本的实验，其优异表现证明了TextSnake 在处理曲形文本方面的有效性。</p>
<p>ICDAR 2015 上进行的是有关偶然场景文本的实验。在单一尺度测试中，TextSnake 超越了绝大多数现有方法（包括那些在多尺度中评估的方法），这证明了 TextSnake 的通用性以及已经可用于复杂场景的多方向文本。</p>
<p>本文在 MSRA-TD500 上进行有关长直文本线的实验。其中 TextSnake 的 F 值 78.3% 优于其他方法。</p>
<h3 id="discussion"><a class="markdownIt-Anchor" href="#discussion"></a> Discussion</h3>
<ul>
<li>想法十分新颖，TextSnake 对文本实例的精确描述具有强大的能力，因此能够带来很大的提升。</li>
<li>论文中提到，TextSnake 进行表征还有另外一个好处，能够很方便的转换成规范的形式，有利于后续的文本识别过程。</li>
<li>后处理操作过于繁琐，并且顶端的边界包裹不准确。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[1] [2018-ECCV] TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes <a href="https://arxiv.org/pdf/1807.01544.pdf"><code>paper</code></a></li>
<li>[2] blog: <a href="https://zhuanlan.zhihu.com/p/40864789">旷视科技提出TextSnake：检测任意形状文本的灵活表征</a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Computer Vision</category>
        <category>Scene Text</category>
      </categories>
      <tags>
        <tag>Scene Text Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>ID-Reveal: Identity-aware DeepFake Video Detection</title>
    <url>/2021/01/05/ID-Reveal-Identity-aware-DeepFake-Video-Detection/</url>
    <content><![CDATA[<p>主要动机：现有的方法通常在特定伪造方法的数据集上进行训练，在未见过的伪造方法数据上泛化性较差。</p>
<a id="more"></a>
<p>核心贡献：如图1所示，论文展现了一种新的鉴伪任务场景，给定一个人物的多个参考视频，判断这个人物的新视频是否伪造。利用自监督学习范式，训练了一个 ID-Reveal 模型进行人脸视频鉴伪。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210105151535.PNG"/>
</div>
<h3 id="id-reveal"><a class="markdownIt-Anchor" href="#id-reveal"></a> ID-Reveal</h3>
<p>该方法先利用3DMM提取一个视频中的人脸特征，再使用一个时序ID网络提取一个128维身份编码，再计算与另一个视频中身份编码的相似性，超过一定阈值即判定为同一个人物。一个人物的伪造数据由3DMM生成网络合成，与时序ID网络进行对抗训练。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210105151926.PNG"/>
</div>
<h3 id="my-review"><a class="markdownIt-Anchor" href="#my-review"></a> My Review</h3>
<ul>
<li>使用3DMM提取后的特征进行鉴伪，不会丢失很多纹理信息吗？而纹理信息是鉴伪任务的重要线索。也许正是这样做，才使得模型关注轮廓上的差异，在低质量数据集上表现更好。</li>
<li>这种基于已知人物参考数据的鉴伪方法是一个值得探究研究点，符合实际应用场景。</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[2020-12] ID-Reveal: Identity-aware DeepFake Video Detection <a href="https://arxiv.org/abs/2012.02512"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Media Forensics</category>
        <category>Face Forgery Detection</category>
      </categories>
      <tags>
        <tag>Face Forgery Detection</tag>
      </tags>
  </entry>
  <entry>
    <title>熵</title>
    <url>/2021/01/06/%E7%86%B5/</url>
    <content><![CDATA[<p>熵的物理意义是体系混乱程度的度量。熵可以用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大。<br><a id="more"></a></p>
<h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>香农对信息的定义：信息是用来消除随机不确定性的东西。</p>
<p>1948年，香农将统计物理中熵的概念引入到信道通信的过程中，定义为信息熵。信息熵是接受的每条消息中包含的信息的平均值，度量信息的不确定性，熵越大，信源的分布越随机。数学上，信息熵其实是信息量的期望。</p>
<script type="math/tex; mode=display">
H(X) = - \sum_{i=1}^{m} p_i(x) \log p_i(x)</script><p>熵的单位为比特(bit)。</p>
<p>信息熵有三条性质：</p>
<ul>
<li>单调性，即发生概率越高的事件，其所携带的信息熵越低；</li>
<li>非负性，即信息熵不能为负，因此在 $\log$ 前添加负号；</li>
<li>累加性，即多个随机事件同时发生存在的总不确定性的量度可以表示为各事件不确定的量度的和。</li>
</ul>
<h3 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h3><p>联合熵(Joint Entropy)就是度量一个联合分布的随机系统的不确定度。分布为$p(x,y)$的一对随机变量，其联合熵的定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
H(X,Y) &= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log p(x,y) \\
&= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log p(x)P(y|x) \\
&= - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log p(x)- \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log P(y|x) \\
&= - \sum_{x \in \mathcal{X}} p(x) \log p(x) -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log P(y|x) \\
&= H(X) + H(Y|X)
\end{aligned}</script><p>注：<script type="math/tex">- \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log p(x)</script> 通过边缘化 $y$ 得到 $- \sum_{x \in \mathcal{X}} p(x) \log p(x)$</p>
<h3 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h3><p>条件熵(Conditional Entropy)$H(Y|X)$，表示在已知随机变量$X$的条件下随机变量$Y$的不确定性，其定义为：</p>
<script type="math/tex; mode=display">
H(Y|X)= -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log P(y|x)</script><h3 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h3><p>对于两个随机变量 $X$ 和 $Y$, 如果其联合分布为$p(x,y)$，边缘分布为$p(x),p(y)$，其互信息可以定义为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
I(X,Y) &=H(X)-H(X|Y) \\
&= - \sum_{x \in \mathcal{X}} p(x) \log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log P(x|y) \\
&=  - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log p(x) + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}  p(x,y) \log P(x|y) \\
&= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x|y)}{p(x)} \\
&= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\end{aligned}</script><p>因此：$H(X)-H(X|Y)=H(Y)-H(Y|X)$。</p>
<p>互信息$I(X,Y)$表示为知道事实$Y$后，原来信息量减少了多少。</p>
<h3 id="相对熵"><a href="#相对熵" class="headerlink" title="相对熵"></a>相对熵</h3><p>相对熵(Relative Entropy)又称为KL 散度(Kullback-Leibler divergence), 是两个概率分布 P 和 Q 差别的非对称性的度量。假设 P 为观察得到的概率分布，Q 为另一种概率分布来近似 P，它们之间的 KL 散度为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
KL(P || Q) &= \sum_{i=1}^{N} P(x_i) \log \frac{P(x_i)}{Q(x_i)} \\
&= \sum_{i=1}^{N} P(x_i)(\log P(x_i) - \log Q(x_i)) \\
&= \sum_{i=1}^{N} P(x_i)\log P(x_i)- \sum_{i=1}^{N} P(x_i)\log Q(x_i) \\
&= -H(P) + H_P(Q)
\end{aligned}</script><p>$H_P(Q)$表示在P分布下，使用Q进行编码需要的比特，$H(P)$表示对分布P所需要的最小比特。因此，$KL(P || Q)$ 的物理意义是使用Q分布进行编码相对于分布P进行编码所多出来的比特。需要注意的 KL 散度是非对称的，$KL(P||Q)  \neq  KL(Q||P)$。</p>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>交叉熵(Cross Entropy)的公式为：</p>
<script type="math/tex; mode=display">
H(P,Q)=-\sum_{i=1}^nP(x_i)log(Q(x_i))</script><p>即为相对熵中的$H_P(Q)$。相对熵=交叉熵-信息熵。在机器学习，需要评估真实分布和预测分布之间的差距，可以使用相对熵进行度量差异，而相对熵中的$H(P)$是不变的，因此，一般直接使用交叉熵衡量两个分布的差异以评估模型。</p>
<h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原(Maximum entropy principle)，认为学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。</p>
<p>换句话，最大熵原理遵循：</p>
<ul>
<li>满足已知信息（约束条件）</li>
<li>不做任何未知假设（剩下的等概率）</li>
</ul>
<p>有点像俗语“不要把鸡蛋放在同一个篮子里”。</p>
<h3 id="最小熵原理"><a href="#最小熵原理" class="headerlink" title="最小熵原理"></a>最小熵原理</h3><p>最小熵原理是一个无监督学习的原理，“熵”就是学习成本，而降低学习成本是我们的不懈追求，所以通过“最小化学习成本”就能够无监督地学习出很多符合我们认知的结果，这就是最小熵原理的基本理念。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li>信息，熵，联合熵 <a href="https://www.cnblogs.com/wqbin/p/12752610.html"><code>page</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Maths in Machine Learning</category>
      </categories>
      <tags>
        <tag>Maths</tag>
      </tags>
  </entry>
  <entry>
    <title>Face Forgery Detection by 3D Decomposition</title>
    <url>/2021/01/06/Face-Forgery-Detection-by-3D-Decomposition/</url>
    <content><![CDATA[<p>自动化所雷震老师团队的工作，主要是引入人脸的3D重建中的信息辅助人脸鉴伪。</p>
<a id="more"></a>
<p>核心动机：Introduction 介绍了一堆，但没有get到点，应该是说光照信息和纹理信息对人脸有一定的帮助，所以就这么做了。</p>
<p>主要贡献：1）用 3DFFA 对人脸进行分解，提取 direct light 和 identity texture 信息，如图1所示；2）提出一个双流模型，同时输入RGB空间和前面提取的信息进行鉴伪。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210106193920.PNG"/>
</div>
<h3 id="双流鉴伪模型"><a class="markdownIt-Anchor" href="#双流鉴伪模型"></a> 双流鉴伪模型</h3>
<p>双流鉴伪模型的设计很基本，中间加了个注意力层，分别有三种融合方式：Score，Feature 和 Halfway 融合。</p>
<div style="text-align: center">
<img src="https://cpblogs.oss-cn-beijing.aliyuncs.com/CPBlogs/20210106194149.PNG"/>
</div>
<h3 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h3>
<ul>
<li>表1实验各种解构后的特征组合对鉴伪的效果；</li>
<li>表2证明了双流模型的有效性；</li>
<li>表3做了注意力机制的消融实验；</li>
<li>表4是解构特征的一些正则操作的消融实验；</li>
<li>表5,6是和其他方法的比较，比较困惑的是，论文中的其他方法的数据应该是自己跑的，但是没有详细说是怎么做的，说服力有点不够。</li>
</ul>
<h3 id="my-review"><a class="markdownIt-Anchor" href="#my-review"></a> My Review</h3>
<ul>
<li>和其他方法的比较不能够令人信服，需要进一步联系作者了解一下；</li>
<li>测试时，对于有些不能重构的人脸是如何处理的？</li>
</ul>
<h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3>
<ul>
<li>[2020-11] Face Forgery Detection by 3D Decomposition <a href="https://arxiv.org/abs/2011.09737"><code>paper</code></a></li>
</ul>
]]></content>
      <categories>
        <category>术业专攻</category>
        <category>Media Forensics</category>
        <category>Face Forgery Detection</category>
      </categories>
      <tags>
        <tag>Face Forgery Detection</tag>
      </tags>
  </entry>
</search>
